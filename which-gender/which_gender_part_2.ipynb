{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Gender Based On Blog Text - Part 2\n",
    "\n",
    "A comparison of a few solutions for identifying the gender of blog authors based on his/her writing style. It is based on a dataset containing 681288 blog posts downloaded from <a href=\"http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm\" target=\"_blank\">here</a>.\n",
    "\n",
    "I am using a smaller dataset containing 145044 blog posts written by 3074 authors (1716 female, 1358 male) in the 24-25 age group.\n",
    "\n",
    "This is the 2nd part in a series of notebooks. I address some of the TODOs from part 1 (data preparation, text preprocessing, etc.) and then\n",
    "1. First, I use the *Bag of Words* (BoW) approach to compare a few classifiers\n",
    "2. Next, I use `GloVe small` **word vector** representation file (`small` has 400K words, `medium` has 1.9M words)\n",
    "3. Next, I train **`Word2Vec`** models on the blog text - both *continuous bag-of-word* (CBOW) and *skip-gram* (SG) models (hierarchical softmax and negative sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current PID : 29565\n",
      "Memory : 15.67 GB\n",
      "#cores : 8\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from gensim import parsing, utils\n",
    "\n",
    "import ast\n",
    "import gc\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import platform\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "import yaml\n",
    "\n",
    "from genderpredictutils import dataprep, textpreprocess, trainingutils\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, LabeledSentence # I have version 0.12.4 installed\n",
    "from operator import itemgetter\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score, train_test_split\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import RidgeClassifier, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import LabelEncoder, Imputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from tabulate import tabulate\n",
    "\n",
    "_random_state = 371250\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "pd.set_option(\"display.max_colwidth\", 300)\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "with open(\"which_gender.yml\", \"r\") as f:\n",
    "    cfg = yaml.load(f)\n",
    "\n",
    "print(\"Current PID : {}\".format(os.getpid()))\n",
    "if platform.system() == 'Linux':\n",
    "    mem_bytes = os.sysconf('SC_PAGE_SIZE') * os.sysconf('SC_PHYS_PAGES')  # e.g. 4015976448\n",
    "    mem_gib = mem_bytes/(1024.**3)\n",
    "    print(\"Memory : {:.2f} GB\".format(mem_gib))\n",
    "print(\"#cores : {}\".format(num_cores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading the dataset\n",
    "Each author's posts appear as a separate file. The name indicates blogger id#, self-provided gender, age, industry and astrological sign. (All are labeled for gender and age but for many, industry and/or sign is marked as unknown.)\n",
    "\n",
    "The work for reading the XML files from the `.zip` file has been done by the `dataprep` module.\n",
    "So, just reusing the pre-created dataset and filtering out authors not in the 24-25 age group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir = cfg[\"common\"][\"data_dir\"]\n",
    "models_dir = cfg[\"common\"][\"models_dir\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Read the `gz` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145044, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path_1 = os.path.join(data_dir, \"blog_posts_metadata.txt.gz\")\n",
    "file_path_2 = os.path.join(data_dir, \"blog_posts.txt.gz\")\n",
    "\n",
    "if os.path.exists(file_path_1) == False or os.path.exists(file_path_2) == False:\n",
    "    print(\"One (or both) of {} or {} does not exist, so creating them\".format(file_path_1, file_path_2))\n",
    "    file_paths = dataprep.prepare_data(data_dir, num_processes=6) # This takes a while\n",
    "    file_path_1, file_path_2 = file_paths[0], file_paths[1]\n",
    "\n",
    "df0 = pd.read_csv(file_path_1, usecols=[\"blogger_id\", \"gender\", \"age\"], sep=\"\\t\", index_col=False)\n",
    "target_age_grp = df0[df0[\"age\"].isin([24,25])][\"blogger_id\"].values.tolist()\n",
    "\n",
    "df_iter = pd.read_csv(file_path_2, sep=\"\\t\", index_col=False, iterator=True, chunksize=1000)\n",
    "df = pd.concat([chunk[chunk[\"blogger_id\"].isin(target_age_grp)] for chunk in df_iter])\n",
    "df = pd.merge(df0, df)\n",
    "df = df.dropna(subset=[\"blog_post\"])\n",
    "df = df.sort_values(by=\"blogger_id\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145044, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get rid of columns we do not need\n",
    "df = df[[\"blogger_id\", \"gender\", \"date\", \"blog_post\"]]\n",
    "num_unreachable_objects = gc.collect()\n",
    "df = df.dropna(subset=[\"blog_post\"])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blogger_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>date</th>\n",
       "      <th>blog_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>119689</th>\n",
       "      <td>5114</td>\n",
       "      <td>male</td>\n",
       "      <td>2002-11-06</td>\n",
       "      <td>Sign #249  urlLink CNN  needs some sense slapped into them:  it's the day after an election, Republicans have taken control the Senate, Dick Gephardt is stepping down as House minority leader and right now, on Larry King, they're talking about the Winona Ryder verdict.  At least they took down t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119722</th>\n",
       "      <td>5114</td>\n",
       "      <td>male</td>\n",
       "      <td>2004-04-19</td>\n",
       "      <td>The new issue of Mindjack is  urlLink now online . In this issue:  urlLink Linked Out: blogging, equality and the future  by Melanie McBride. Plus  urlLink Kill Bill Vol. 2  reviewed by Jesse Walker.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119723</th>\n",
       "      <td>5114</td>\n",
       "      <td>male</td>\n",
       "      <td>2004-04-12</td>\n",
       "      <td>There's a  urlLink new issue  of Mindjack now online.  In it, the first article from Mindjack's newest contributor J.D. Lasica; he writes about copyright law in  urlLink \"The Killing Fields\" .  Also, I review DVDs of  urlLink Breathless, Russian Ark, and Z .  Spread the word.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        blogger_id gender        date  \\\n",
       "119689        5114   male  2002-11-06   \n",
       "119722        5114   male  2004-04-19   \n",
       "119723        5114   male  2004-04-12   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                          blog_post  \n",
       "119689  Sign #249  urlLink CNN  needs some sense slapped into them:  it's the day after an election, Republicans have taken control the Senate, Dick Gephardt is stepping down as House minority leader and right now, on Larry King, they're talking about the Winona Ryder verdict.  At least they took down t...  \n",
       "119722                                                                                                      The new issue of Mindjack is  urlLink now online . In this issue:  urlLink Linked Out: blogging, equality and the future  by Melanie McBride. Plus  urlLink Kill Bill Vol. 2  reviewed by Jesse Walker.  \n",
       "119723                         There's a  urlLink new issue  of Mindjack now online.  In it, the first article from Mindjack's newest contributor J.D. Lasica; he writes about copyright law in  urlLink \"The Killing Fields\" .  Also, I review DVDs of  urlLink Breathless, Russian Ark, and Z .  Spread the word.  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Encode the gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['female', 'male']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blogger_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>date</th>\n",
       "      <th>blog_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>119689</th>\n",
       "      <td>5114</td>\n",
       "      <td>1</td>\n",
       "      <td>2002-11-06</td>\n",
       "      <td>Sign #249  urlLink CNN  needs some sense slapped into them:  it's the day after an election, Republicans have taken control the Senate, Dick Gephardt is stepping down as House minority leader and right now, on Larry King, they're talking about the Winona Ryder verdict.  At least they took down t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119722</th>\n",
       "      <td>5114</td>\n",
       "      <td>1</td>\n",
       "      <td>2004-04-19</td>\n",
       "      <td>The new issue of Mindjack is  urlLink now online . In this issue:  urlLink Linked Out: blogging, equality and the future  by Melanie McBride. Plus  urlLink Kill Bill Vol. 2  reviewed by Jesse Walker.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119723</th>\n",
       "      <td>5114</td>\n",
       "      <td>1</td>\n",
       "      <td>2004-04-12</td>\n",
       "      <td>There's a  urlLink new issue  of Mindjack now online.  In it, the first article from Mindjack's newest contributor J.D. Lasica; he writes about copyright law in  urlLink \"The Killing Fields\" .  Also, I review DVDs of  urlLink Breathless, Russian Ark, and Z .  Spread the word.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        blogger_id  gender        date  \\\n",
       "119689        5114       1  2002-11-06   \n",
       "119722        5114       1  2004-04-19   \n",
       "119723        5114       1  2004-04-12   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                          blog_post  \n",
       "119689  Sign #249  urlLink CNN  needs some sense slapped into them:  it's the day after an election, Republicans have taken control the Senate, Dick Gephardt is stepping down as House minority leader and right now, on Larry King, they're talking about the Winona Ryder verdict.  At least they took down t...  \n",
       "119722                                                                                                      The new issue of Mindjack is  urlLink now online . In this issue:  urlLink Linked Out: blogging, equality and the future  by Melanie McBride. Plus  urlLink Kill Bill Vol. 2  reviewed by Jesse Walker.  \n",
       "119723                         There's a  urlLink new issue  of Mindjack now online.  In it, the first article from Mindjack's newest contributor J.D. Lasica; he writes about copyright law in  urlLink \"The Killing Fields\" .  Also, I review DVDs of  urlLink Breathless, Russian Ark, and Z .  Spread the word.  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_enc = LabelEncoder()\n",
    "gender_enc.fit(df.gender.values.tolist())\n",
    "print list(gender_enc.classes_)\n",
    "df[\"gender\"] = gender_enc.transform(df.gender.values.tolist())\n",
    "df.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Preprocess the text\n",
    "This takes a while. I noticed **each instance** of Spacy English parser takes up **~3GB of RAM** (also verified it, https://github.com/spacy-io/spaCy/issues/100), so set the number of processes prudently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading tokenized_text.txt\n",
      "CPU times: user 33.5 s, sys: 2.27 s, total: 35.8 s\n",
      "Wall time: 35.8 s\n",
      "(145044, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blogger_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>date</th>\n",
       "      <th>blog_post</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5114</td>\n",
       "      <td>1</td>\n",
       "      <td>2002-11-06</td>\n",
       "      <td>Sign #249  urlLink CNN  needs some sense slapped into them:  it's the day after an election, Republicans have taken control the Senate, Dick Gephardt is stepping down as House minority leader and right now, on Larry King, they're talking about the Winona Ryder verdict.  At least they took down t...</td>\n",
       "      <td>[sign, 249, cnn, need, sense, slap, day, election, republicans, control, senate, dick, gephardt, step, house, minority, leader, right, larry, king, 're, talk, winona, ryder, verdict, breaking, news, graphic]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5114</td>\n",
       "      <td>1</td>\n",
       "      <td>2004-04-19</td>\n",
       "      <td>The new issue of Mindjack is  urlLink now online . In this issue:  urlLink Linked Out: blogging, equality and the future  by Melanie McBride. Plus  urlLink Kill Bill Vol. 2  reviewed by Jesse Walker.</td>\n",
       "      <td>[new, issue, mindjack, online, link, blogging, equality, future, melanie, mcbride, plus, kill, vol, 2, review, jesse, walker]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5114</td>\n",
       "      <td>1</td>\n",
       "      <td>2004-04-12</td>\n",
       "      <td>There's a  urlLink new issue  of Mindjack now online.  In it, the first article from Mindjack's newest contributor J.D. Lasica; he writes about copyright law in  urlLink \"The Killing Fields\" .  Also, I review DVDs of  urlLink Breathless, Russian Ark, and Z .  Spread the word.</td>\n",
       "      <td>[new, issue, mindjack, online, article, mindjack, 's, new, contributor, j.d., lasica, write, copyright, law, killing, fields, review, dvd, breathless, russian, ark, z, spread, word]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   blogger_id  gender        date  \\\n",
       "0        5114       1  2002-11-06   \n",
       "1        5114       1  2004-04-19   \n",
       "2        5114       1  2004-04-12   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     blog_post  \\\n",
       "0  Sign #249  urlLink CNN  needs some sense slapped into them:  it's the day after an election, Republicans have taken control the Senate, Dick Gephardt is stepping down as House minority leader and right now, on Larry King, they're talking about the Winona Ryder verdict.  At least they took down t...   \n",
       "1                                                                                                      The new issue of Mindjack is  urlLink now online . In this issue:  urlLink Linked Out: blogging, equality and the future  by Melanie McBride. Plus  urlLink Kill Bill Vol. 2  reviewed by Jesse Walker.   \n",
       "2                         There's a  urlLink new issue  of Mindjack now online.  In it, the first article from Mindjack's newest contributor J.D. Lasica; he writes about copyright law in  urlLink \"The Killing Fields\" .  Also, I review DVDs of  urlLink Breathless, Russian Ark, and Z .  Spread the word.   \n",
       "\n",
       "                                                                                                                                                                                                    tokenized_text  \n",
       "0  [sign, 249, cnn, need, sense, slap, day, election, republicans, control, senate, dick, gephardt, step, house, minority, leader, right, larry, king, 're, talk, winona, ryder, verdict, breaking, news, graphic]  \n",
       "1                                                                                    [new, issue, mindjack, online, link, blogging, equality, future, melanie, mcbride, plus, kill, vol, 2, review, jesse, walker]  \n",
       "2                            [new, issue, mindjack, online, article, mindjack, 's, new, contributor, j.d., lasica, write, copyright, law, killing, fields, review, dvd, breathless, russian, ark, z, spread, word]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = \"tokenized_text.txt\"\n",
    "if os.path.exists(tokenized_dataset) == False:\n",
    "    %time df = textpreprocess.tokenize_text(df, col_name=\"blog_post\", num_processes=cfg[\"tokenize_text\"][\"num_processes\"])\n",
    "    df.to_csv(tokenized_dataset, sep=\"\\t\", index=False)\n",
    "else:\n",
    "    print(\"Reading {}\".format(tokenized_dataset))\n",
    "    %time df = pd.read_csv(tokenized_dataset, sep=\"\\t\", converters={\"tokenized_text\":ast.literal_eval})\n",
    "\n",
    "if len(df.columns.values.tolist()) >= 6:\n",
    "    df.drop([\"Unnamed: 0\"], axis=1, inplace=True)\n",
    "\n",
    "print(df.shape)\n",
    "df.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.24 s, sys: 296 ms, total: 7.53 s\n",
      "Wall time: 7.54 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blogger_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>date</th>\n",
       "      <th>blog_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5114</td>\n",
       "      <td>1</td>\n",
       "      <td>2002-11-06</td>\n",
       "      <td>sign 249 cnn need sense slap day election republicans control senate dick gephardt step house minority leader right larry king 're talk winona ryder verdict breaking news graphic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5114</td>\n",
       "      <td>1</td>\n",
       "      <td>2004-04-19</td>\n",
       "      <td>new issue mindjack online link blogging equality future melanie mcbride plus kill vol 2 review jesse walker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5114</td>\n",
       "      <td>1</td>\n",
       "      <td>2004-04-12</td>\n",
       "      <td>new issue mindjack online article mindjack 's new contributor j.d. lasica write copyright law killing fields review dvd breathless russian ark z spread word</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   blogger_id  gender        date  \\\n",
       "0        5114       1  2002-11-06   \n",
       "1        5114       1  2004-04-19   \n",
       "2        5114       1  2004-04-12   \n",
       "\n",
       "                                                                                                                                                                            blog_post  \n",
       "0  sign 249 cnn need sense slap day election republicans control senate dick gephardt step house minority leader right larry king 're talk winona ryder verdict breaking news graphic  \n",
       "1                                                                         new issue mindjack online link blogging equality future melanie mcbride plus kill vol 2 review jesse walker  \n",
       "2                        new issue mindjack online article mindjack 's new contributor j.d. lasica write copyright law killing fields review dvd breathless russian ark z spread word  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the tokens into a single string - as required downstream\n",
    "def func_concat_tokens(x):\n",
    "    terms = x[\"tokenized_text\"]\n",
    "    terms = [str(t) for t in terms]\n",
    "    return \" \".join(terms)\n",
    "%time df[\"tokenized_text_rejoined\"] = df.apply(func_concat_tokens , axis=1)\n",
    "\n",
    "# Next, replace the text within \"blog_post\" with text in \"tokenized_text_rejoined\"\n",
    "df[\"blog_post\"] = df[\"tokenized_text_rejoined\"]\n",
    "df.drop([\"tokenized_text\", \"tokenized_text_rejoined\"], axis=1, inplace=True)\n",
    "del textpreprocess._spacy_parser_\n",
    "num_unreachable_objects = gc.collect()\n",
    "df.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Feature Extraction\n",
    "How many features do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.7 s, sys: 790 ms, total: 18.5 s\n",
      "Wall time: 18.5 s\n",
      "min_df = 0.001, X.shape : (145044, 6739), len(y) : 145044\n",
      "CPU times: user 17.7 s, sys: 899 ms, total: 18.6 s\n",
      "Wall time: 18.6 s\n",
      "min_df = 0.005, X.shape : (145044, 2039), len(y) : 145044\n",
      "CPU times: user 17.9 s, sys: 731 ms, total: 18.6 s\n",
      "Wall time: 18.7 s\n",
      "min_df = 0.010, X.shape : (145044, 1091), len(y) : 145044\n",
      "CPU times: user 18.6 s, sys: 879 ms, total: 19.5 s\n",
      "Wall time: 19.5 s\n",
      "max_df = 0.100, X.shape : (145044, 239874), len(y) : 145044\n",
      "CPU times: user 18.7 s, sys: 891 ms, total: 19.6 s\n",
      "Wall time: 19.6 s\n",
      "max_df = 0.200, X.shape : (145044, 239919), len(y) : 145044\n",
      "CPU times: user 18.8 s, sys: 819 ms, total: 19.6 s\n",
      "Wall time: 19.6 s\n",
      "max_df = 0.300, X.shape : (145044, 239931), len(y) : 145044\n",
      "CPU times: user 18.7 s, sys: 987 ms, total: 19.7 s\n",
      "Wall time: 19.7 s\n",
      "max_df = 0.400, X.shape : (145044, 239936), len(y) : 145044\n",
      "CPU times: user 18.6 s, sys: 979 ms, total: 19.6 s\n",
      "Wall time: 19.6 s\n",
      "max_df = 0.500, X.shape : (145044, 239936), len(y) : 145044\n"
     ]
    }
   ],
   "source": [
    "# Iterate through multiple values of min_df and max_df\n",
    "min_dfs = [0.001, 0.005, 0.01]\n",
    "for min_df in min_dfs:\n",
    "    vectorizer = TfidfVectorizer(sublinear_tf=True, min_df=min_df, stop_words=\"english\")\n",
    "    %time X = vectorizer.fit_transform(df[\"blog_post\"]) # sparse matrix in CSR format\n",
    "    y = np.array(df.gender.values.tolist())\n",
    "    print \"min_df = {:.3f}, X.shape : {}, len(y) : {}\".format(min_df, X.shape, len(y))\n",
    "    del vectorizer, X, y\n",
    "    num_unreachable_objects = gc.collect()\n",
    "\n",
    "max_dfs = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "for max_df in max_dfs:\n",
    "    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=max_df, stop_words=\"english\")\n",
    "    %time X = vectorizer.fit_transform(df[\"blog_post\"]) # sparse matrix in CSR format\n",
    "    y = np.array(df.gender.values.tolist())\n",
    "    print \"max_df = {:.3f}, X.shape : {}, len(y) : {}\".format(max_df, X.shape, len(y))\n",
    "    del vectorizer, X, y\n",
    "    num_unreachable_objects = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran TF-IDF vectorizer with minimum document frequency 0.1%, 0.5% and 1.0% to get 6739, 2039 and 1091 features respectively. I then repeated with maximum document frequency in the range 10 - 50%, to get ~240K features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.8 s, sys: 734 ms, total: 19.6 s\n",
      "Wall time: 19.6 s\n",
      "max_df = 0.5, X.shape : (145044, 239936), len(y) : 145044\n"
     ]
    }
   ],
   "source": [
    "# Choosing max_df=0.5, which gives ~240K features - feature selection will be done later\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words=\"english\")\n",
    "%time X = vectorizer.fit_transform(df[\"blog_post\"]) # sparse matrix in CSR format\n",
    "y = np.array(df.gender.values.tolist())\n",
    "print \"max_df = 0.5, X.shape : {}, len(y) : {}\".format(X.shape, len(y))\n",
    "num_unreachable_objects = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Feature Selection\n",
    "Using recursive feature elimination to select the top 20K and 30K features. 30K might increase memory requirements for downstream approaches, such as `GloVe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape (before feature selection) : (145044, 239936)\n",
      "Feature Selection : Top 20K\n",
      "CPU times: user 46.9 s, sys: 1.78 s, total: 48.6 s\n",
      "Wall time: 48.7 s\n",
      "X20K.shape : (145044, 20000)\n",
      "Feature Selection : Top 30K\n",
      "CPU times: user 45 s, sys: 1.8 s, total: 46.9 s\n",
      "Wall time: 46.9 s\n",
      "X30K.shape : (145044, 30000)\n"
     ]
    }
   ],
   "source": [
    "print(\"X.shape (before feature selection) : {}\".format(X.shape))\n",
    "print(\"Feature Selection : Top 20K\")\n",
    "feature_selector = RFE(estimator=LinearSVC(C=0.1, random_state=_random_state), n_features_to_select=20000, step=0.05)\n",
    "%time X20K = feature_selector.fit_transform(X, y)\n",
    "print(\"X20K.shape : {}\".format(X20K.shape))\n",
    "print(\"Feature Selection : Top 30K\")\n",
    "feature_selector = RFE(estimator=LinearSVC(C=0.1, random_state=_random_state), n_features_to_select=30000, step=0.05)\n",
    "%time X30K = feature_selector.fit_transform(X, y)\n",
    "print(\"X30K.shape : {}\".format(X30K.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bag of Words (BoW) Approach\n",
    "Training a few classifiers and compare their F1-scores. Doing this for both the 20K and 30K feature datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model                                  F1-score\n",
      "-----------------------------------  ----------\n",
      "MultinomialNB_01, BOW, 20K               0.7307\n",
      "LinearSVC_10, BOW, 20K                   0.7296\n",
      "MultinomialNB_02, BOW, 20K               0.7294\n",
      "RidgeClassifier-auto-1e-3, BOW, 20K      0.7293\n",
      "RidgeClassifier-auto-1e-4, BOW, 20K      0.7293\n",
      "LinearSVC_07, BOW, 20K                   0.7289\n",
      "MultinomialNB_04, BOW, 20K               0.7277\n",
      "LinearSVC_04, BOW, 20K                   0.7276\n",
      "MultinomialNB_06, BOW, 20K               0.7258\n",
      "MultinomialNB_08, BOW, 20K               0.7249\n",
      "PassiveAggressive_01, BOW, 20K           0.7246\n",
      "MultinomialNB_10, BOW, 20K               0.7240\n",
      "MultinomialNB_01, BOW, 30K               0.7235\n",
      "LinearSVC_01, BOW, 20K                   0.7223\n",
      "MultinomialNB_15, BOW, 20K               0.7220\n",
      "RidgeClassifier-auto-1e-4, BOW, 30K      0.7218\n",
      "RidgeClassifier-auto-1e-3, BOW, 30K      0.7218\n",
      "LinearSVC_10, BOW, 30K                   0.7215\n",
      "SGD_l2_penalty, BOW, 20K                 0.7213\n",
      "LogisticRegression, BOW, 20K             0.7211\n",
      "MultinomialNB_02, BOW, 30K               0.7210\n",
      "LinearSVC_07, BOW, 30K                   0.7209\n",
      "LinearSVC_04, BOW, 30K                   0.7201\n",
      "MultinomialNB_04, BOW, 30K               0.7185\n",
      "SGD_l2_penalty, BOW, 30K                 0.7182\n",
      "PassiveAggressive_01, BOW, 30K           0.7173\n",
      "LinearSVC_01, BOW, 30K                   0.7168\n",
      "MultinomialNB_06, BOW, 30K               0.7168\n",
      "LogisticRegression, BOW, 30K             0.7155\n",
      "MultinomialNB_08, BOW, 30K               0.7152\n",
      "SGD_elasticnet_penalty, BOW, 20K         0.7150\n",
      "MultinomialNB_10, BOW, 30K               0.7145\n",
      "MultinomialNB_15, BOW, 30K               0.7129\n",
      "SGD_elasticnet_penalty, BOW, 30K         0.7128\n",
      "SGD_l1_penalty, BOW, 20K                 0.7074\n",
      "SGD_l1_penalty, BOW, 30K                 0.7067\n"
     ]
    }
   ],
   "source": [
    "# Run BoW using new method\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clfs = [\n",
    "    #(\"ExtraTrees_200, BOW\", ExtraTreesClassifier(n_estimators=200, random_state=_random_state)), # Takes too long\n",
    "    (\"LinearSVC_01, BOW\", LinearSVC(C=0.1, random_state=_random_state)),\n",
    "    (\"LinearSVC_04, BOW\", LinearSVC(C=0.4, random_state=_random_state)),\n",
    "    (\"LinearSVC_07, BOW\", LinearSVC(C=0.7, random_state=_random_state)),\n",
    "    (\"LinearSVC_10, BOW\", LinearSVC(C=1.0, random_state=_random_state)),\n",
    "    (\"LogisticRegression, BOW\", LogisticRegression()),\n",
    "    (\"MultinomialNB_01, BOW\", MultinomialNB(alpha=.10)),\n",
    "    (\"MultinomialNB_02, BOW\", MultinomialNB(alpha=.20)),\n",
    "    (\"MultinomialNB_04, BOW\", MultinomialNB(alpha=.40)),\n",
    "    (\"MultinomialNB_06, BOW\", MultinomialNB(alpha=.60)),\n",
    "    (\"MultinomialNB_08, BOW\", MultinomialNB(alpha=.80)),\n",
    "    (\"MultinomialNB_10, BOW\", MultinomialNB(alpha=1.0)),\n",
    "    (\"MultinomialNB_15, BOW\", MultinomialNB(alpha=1.5)),\n",
    "    (\"PassiveAggressive_01, BOW\", PassiveAggressiveClassifier(C=0.1, n_iter=50, random_state=_random_state)),\n",
    "    (\"RidgeClassifier-auto-1e-3, BOW\", RidgeClassifier(tol=1e-3, solver=\"auto\", random_state=_random_state)),\n",
    "    (\"RidgeClassifier-auto-1e-4, BOW\", RidgeClassifier(tol=1e-4, solver=\"auto\", random_state=_random_state)),\n",
    "    (\"SGD_elasticnet_penalty, BOW\", \n",
    "     SGDClassifier(alpha=.0001, n_iter=150, penalty=\"elasticnet\", random_state=_random_state)),\n",
    "    (\"SGD_l1_penalty, BOW\", SGDClassifier(alpha=.0001, n_iter=150, penalty=\"l1\", random_state=_random_state)),\n",
    "    (\"SGD_l2_penalty, BOW\", SGDClassifier(alpha=.0001, n_iter=150, penalty=\"l2\", random_state=_random_state)),\n",
    "]\n",
    "\n",
    "clfs20K = [(x[0] + \", 20K\", x[1]) for x in clfs]\n",
    "scores_bow = trainingutils.compare_classifiers(clfs20K, X20K, y, n_jobs=num_cores, print_scores=False)\n",
    "\n",
    "clfs30K = [(x[0] + \", 30K\", x[1]) for x in clfs]\n",
    "scores_bow.extend(trainingutils.compare_classifiers(clfs30K, X30K, y, n_jobs=num_cores, print_scores=False))\n",
    "\n",
    "scores_bow = sorted(scores_bow, key=lambda (_, x): -x)\n",
    "print(tabulate(scores_bow, floatfmt=\".4f\", headers=(\"Model\", \"F1-score\")))\n",
    "del clfs, clfs20K, clfs30K\n",
    "num_unreachable_objects = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using Word Vectors\n",
    "\n",
    "\n",
    "### 3.1 Using `GloVe` word vector representation files\n",
    "The **`GloVe`** word vector representation files can be downloaded from http://nlp.stanford.edu/data/ or https://github.com/stanfordnlp/GloVe. There are 3 files : \n",
    "- `glove.6B.zip` (6 Billion tokens, hence '*small*', 400K words) - has 50/100/200/300-dimension vectors,\n",
    "- `glove.42B.300d.zip` (42 Billion tokens, hence '*medium*', 1.9M words),\t\n",
    "- `glove.840B.300d.zip` (840 Billion tokens, hence '*large*', 2.2M words)\n",
    "\n",
    "I just used the `GloVe-small` **100-dimension** word vector representation file - `medium` requires very high RAM usage.\n",
    "\n",
    "Each word in each blog post needs to be mapped to its vector representation - which is accordingly used as features.  This is done using customer vectorizers as first introduced by \n",
    "<a href=\"http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/\" target=\"_blank\">nadbordrozd</a> (`MeanEmbeddingVectorizer` and `TfidfEmbeddingVectorizer`).\n",
    "\n",
    "Using `GloVe-small` and taking the mean of the vectors using the `MeanEmbeddingVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ../../../models/glove.6B.100d.txt.gz\n",
      "400000 keys. First 8 : ['biennials', 'verplank', 'soestdijk', 'woode', 'mdbo', 'sowell', 'mdbu', 'woods']\n",
      "\n",
      "CPU times: user 18.9 s, sys: 464 ms, total: 19.3 s\n",
      "Wall time: 19.5 s\n",
      "CPU times: user 5min 36s, sys: 5min 45s, total: 11min 22s\n",
      "Wall time: 20min 1s\n",
      "CPU times: user 6min 8s, sys: 4min 48s, total: 10min 57s\n",
      "Wall time: 30min 23s\n",
      "Model                                          F1-score\n",
      "-------------------------------------------  ----------\n",
      "LinearSVC_07, GloVe small, 20K                   0.6935\n",
      "MultinomialNB_01, GloVe small, 20K               0.6935\n",
      "RidgeClassifier-auto-1e-3, GloVe small, 20K      0.6935\n",
      "SGD_elasticnet_penalty, GloVe small, 20K         0.6935\n",
      "LinearSVC_07, GloVe small, 30K                   0.6935\n",
      "MultinomialNB_01, GloVe small, 30K               0.6935\n",
      "RidgeClassifier-auto-1e-3, GloVe small, 30K      0.6935\n",
      "SGD_elasticnet_penalty, GloVe small, 30K         0.6935\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%time glove_w2v = trainingutils.read_GloVe_file(os.path.join(models_dir, \"glove.6B.100d.txt.gz\"))\n",
    "\n",
    "ts = time.time()\n",
    "\n",
    "vec = (\"vectorizer\", trainingutils.MeanEmbeddingVectorizer(glove_w2v))\n",
    "\n",
    "clfs = [\n",
    "    (\"LinearSVC_07, GloVe small\", Pipeline([vec, (\"lsvc_07\", LinearSVC(C=0.7, random_state=_random_state))])),\n",
    "    (\"MultinomialNB_01, GloVe small\", Pipeline([vec, (\"mnb_01\", MultinomialNB(alpha=.10))])),\n",
    "    (\"RidgeClassifier-auto-1e-3, GloVe small\",\n",
    "     Pipeline([vec, (\"ridge\", RidgeClassifier(tol=1e-3, solver=\"auto\", random_state=_random_state))])),\n",
    "    (\"SGD_elasticnet_penalty, GloVe small\", \n",
    "     Pipeline([vec, (\"sgd\", SGDClassifier(alpha=.0001, n_iter=150, penalty=\"elasticnet\", random_state=_random_state))])),\n",
    "]\n",
    "\n",
    "# Run for 20K features\n",
    "clfs20K = [(x[0] + \", 20K\", x[1]) for x in clfs]\n",
    "%time scores_glove_w2v = trainingutils.compare_classifiers(clfs20K, X20K, y, n_jobs=num_cores, print_scores=False)\n",
    "\n",
    "# Run for 30K features\n",
    "clfs30K = [(x[0] + \", 30K\", x[1]) for x in clfs]\n",
    "%time scores_glove_w2v.extend(trainingutils.compare_classifiers(clfs30K, X30K, y, n_jobs=num_cores, print_scores=False))\n",
    "\n",
    "scores_glove_w2v = sorted(scores_glove_w2v, key=lambda (_, x): -x)\n",
    "print(tabulate(scores_glove_w2v, floatfmt=\".4f\", headers=(\"Model\", \"F1-score\")))\n",
    "del clfs, clfs20K, clfs30K\n",
    "num_unreachable_objects = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something is not right using the `MeanEmbeddingVectorizer` with the `GloVe small`. Need to revisit this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 `Word2Vec` model on blog text\n",
    "Training `Word2Vec` models on the blog text\n",
    "- both continuous bag-of-word (CBOW) \n",
    "- skip-gram (SG) models (hierarchical softmax and negative sampling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = df.blog_post.values.tolist() # this is a list of strings\n",
    "documents = [unicode(x).split() for x in documents] # this is a list of lists of tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Word2Vec - CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing 100-dimension Word2Vec CBOW model based on text of 145044 blog posts\n",
      "CPU times: user 2min 36s, sys: 7.02 s, total: 2min 43s\n",
      "Wall time: 45.1 s\n",
      "Word2Vec CBOW model : Word2Vec(vocab=41338, size=100, alpha=0.025)\n",
      "CPU times: user 42.3 s, sys: 28.7 s, total: 1min 10s\n",
      "Wall time: 3min 59s\n",
      "CPU times: user 43.3 s, sys: 29.9 s, total: 1min 13s\n",
      "Wall time: 3min 59s\n",
      "Model                                   F1-score\n",
      "------------------------------------  ----------\n",
      "LinearSVC_07, CBOW, 20K                   0.6935\n",
      "MultinomialNB_01, CBOW, 20K               0.6935\n",
      "RidgeClassifier-auto-1e-3, CBOW, 20K      0.6935\n",
      "SGD_elasticnet_penalty, CBOW, 20K         0.6935\n",
      "LinearSVC_07, CBOW, 30K                   0.6935\n",
      "MultinomialNB_01, CBOW, 30K               0.6935\n",
      "RidgeClassifier-auto-1e-3, CBOW, 30K      0.6935\n",
      "SGD_elasticnet_penalty, CBOW, 30K         0.6935\n"
     ]
    }
   ],
   "source": [
    "w2v_dim = 100\n",
    "\n",
    "ts = time.time()\n",
    "print(\"Constructing {}-dimension Word2Vec CBOW model based on text of {} blog posts\".format(w2v_dim, len(documents)))\n",
    "%time w2v = Word2Vec(documents, size=w2v_dim, window=5, min_count=10, sg=0, workers=num_cores)\n",
    "print(\"Word2Vec CBOW model : {}\".format(w2v)) # hs=0, negative=5, cbow_mean=1\n",
    "vec = (\"vec1\", trainingutils.MeanEmbeddingVectorizer({w: vec for w, vec in zip(w2v.index2word, w2v.syn0)}))\n",
    "\n",
    "clfs = [\n",
    "    (\"LinearSVC_07, CBOW\", Pipeline([vec, (\"lsvc_07\", LinearSVC(C=0.7, random_state=_random_state))])),\n",
    "    (\"MultinomialNB_01, CBOW\", Pipeline([vec, (\"mnb_01\", MultinomialNB(alpha=.10))])),\n",
    "    (\"RidgeClassifier-auto-1e-3, CBOW\",\n",
    "     Pipeline([vec, (\"ridge\", RidgeClassifier(tol=1e-3, solver=\"auto\", random_state=_random_state))])),\n",
    "    (\"SGD_elasticnet_penalty, CBOW\", \n",
    "     Pipeline([vec, (\"sgd\", SGDClassifier(alpha=.0001, n_iter=150, penalty=\"elasticnet\", random_state=_random_state))])),\n",
    "]\n",
    "\n",
    "# Run for 20K features\n",
    "clfs20K = [(x[0] + \", 20K\", x[1]) for x in clfs]\n",
    "%time scores_w2v_cbow = trainingutils.compare_classifiers(clfs20K, X20K, y, n_jobs=num_cores, print_scores=False)\n",
    "\n",
    "# Run for 30K features\n",
    "clfs30K = [(x[0] + \", 30K\", x[1]) for x in clfs]\n",
    "%time scores_w2v_cbow.extend(trainingutils.compare_classifiers(clfs30K, X30K, y, n_jobs=num_cores, print_scores=False))\n",
    "\n",
    "scores_w2v_cbow = sorted(scores_w2v_cbow, key=lambda (_, x): -x)\n",
    "print(tabulate(scores_w2v_cbow, floatfmt=\".4f\", headers=(\"Model\", \"F1-score\")))\n",
    "del clfs, vec, w2v\n",
    "num_unreachable_objects = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something is not right using the `MeanEmbeddingVectorizer`. Need to explore other ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Word2Vec - CBOW (Using average feature vectors for each document)\n",
    "This idea (of using average feature vectors for each document) was first introduced during <a href=\"https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-3-more-fun-with-word-vectors\" target=\"_blank\">this</a> competition on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First, free up some RAM\n",
    "del X20K, X30K\n",
    "num_unreachable_objects = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing 100-dimension Word2Vec CBOW model based on text of 145044 blog posts\n",
      "CPU times: user 2min 34s, sys: 10.1 s, total: 2min 44s\n",
      "Wall time: 44.7 s\n",
      "Word2Vec CBOW model : Word2Vec(vocab=41338, size=100, alpha=0.025)\n",
      "Getting the average feature vector for each document ...\n",
      "CPU times: user 12min 15s, sys: 25.9 s, total: 12min 41s\n",
      "Wall time: 12min 42s\n"
     ]
    }
   ],
   "source": [
    "# Next, construct the Word2Vec CBOW model\n",
    "w2v_dim = 100\n",
    "print(\"Constructing {}-dimension Word2Vec CBOW model based on text of {} blog posts\".format(w2v_dim, len(documents)))\n",
    "%time w2v = Word2Vec(documents, size=w2v_dim, window=5, min_count=10, sg=0, workers=num_cores)\n",
    "print(\"Word2Vec CBOW model : {}\".format(w2v))\n",
    "\n",
    "# Get the average feature vector for each document\n",
    "print(\"Getting the average feature vector for each document ...\")\n",
    "%time doc_vecs = trainingutils.get_avg_feature_vectors(documents, w2v)\n",
    "\n",
    "# Handling missing values (if any)\n",
    "# (not doing this will give a \"ValueError: Input contains NaN, infinity\")\n",
    "doc_vecs = Imputer().fit_transform(doc_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model                              F1-score\n",
      "-------------------------------  ----------\n",
      "RidgeClassifier-auto-1e-3, CBOW      0.6718\n",
      "RidgeClassifier-auto-1e-4, CBOW      0.6718\n",
      "LinearSVC_10, CBOW                   0.6717\n",
      "LinearSVC_07, CBOW                   0.6716\n",
      "LinearSVC_04, CBOW                   0.6715\n",
      "LinearSVC_02, CBOW                   0.6714\n",
      "LinearSVC_01, CBOW                   0.6714\n",
      "LogisticRegression, CBOW             0.6713\n",
      "RandomForest_200, CBOW               0.6663\n",
      "SGD_l2_penalty, CBOW                 0.6641\n",
      "SGD_elasticnet_penalty, CBOW         0.6639\n",
      "SGD_l1_penalty, CBOW                 0.6636\n",
      "PassiveAggressive_01, CBOW           0.6123\n",
      "\n",
      "Time Taken : 18.3 minutes\n",
      "\n",
      "CPU times: user 33.6 s, sys: 13.9 s, total: 47.6 s\n",
      "Wall time: 18min 20s\n"
     ]
    }
   ],
   "source": [
    "# Train and compare a few classifiers based on the average feature vectors\n",
    "clfs = [\n",
    "    (\"LinearSVC_01, CBOW\", LinearSVC(C=0.1, random_state=_random_state)),\n",
    "    (\"LinearSVC_02, CBOW\", LinearSVC(C=0.2, random_state=_random_state)),\n",
    "    (\"LinearSVC_04, CBOW\", LinearSVC(C=0.4, random_state=_random_state)),\n",
    "    (\"LinearSVC_07, CBOW\", LinearSVC(C=0.7, random_state=_random_state)),\n",
    "    (\"LinearSVC_10, CBOW\", LinearSVC(C=1.0, random_state=_random_state)),\n",
    "    (\"LogisticRegression, CBOW\", LogisticRegression()),\n",
    "    (\"PassiveAggressive_01, CBOW\", PassiveAggressiveClassifier(C=0.1, n_iter=50, random_state=_random_state)),\n",
    "    (\"RandomForest_200, CBOW\", RandomForestClassifier(n_estimators = 200, random_state=_random_state)),\n",
    "    (\"RidgeClassifier-auto-1e-3, CBOW\", RidgeClassifier(tol=1e-3, solver=\"auto\", random_state=_random_state)),\n",
    "    (\"RidgeClassifier-auto-1e-4, CBOW\", RidgeClassifier(tol=1e-4, solver=\"auto\", random_state=_random_state)),\n",
    "    (\"SGD_elasticnet_penalty, CBOW\", \n",
    "     SGDClassifier(alpha=.0001, n_iter=150, penalty=\"elasticnet\", random_state=_random_state)),\n",
    "    (\"SGD_l1_penalty, CBOW\", SGDClassifier(alpha=.0001, n_iter=150, penalty=\"l1\", random_state=_random_state)),\n",
    "    (\"SGD_l2_penalty, CBOW\", SGDClassifier(alpha=.0001, n_iter=150, penalty=\"l2\", random_state=_random_state)),\n",
    "]\n",
    "\n",
    "%time scores_w2v_cbow = trainingutils.compare_classifiers(clfs, doc_vecs, y, n_jobs=num_cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Word2Vec - Skip-gram (SG) using negative sampling (Using average feature vectors for each document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing 100-dimension Word2Vec model (Skip-gram using negative sampling) based on text of 145044 blog posts\n",
      "CPU times: user 9min 20s, sys: 21.8 s, total: 9min 42s\n",
      "Wall time: 1min 18s\n",
      "Word2Vec SG + NS model : Word2Vec(vocab=41338, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "w2v_dim = 100\n",
    "suffix = \"SG + NS\" # Skip-gram (SG) using negative sampling\n",
    "\n",
    "# Construct the Word2Vec SG + NS model\n",
    "print(\"Constructing {}-dimension Word2Vec model (Skip-gram using negative sampling) based on text of {} blog posts\"\\\n",
    "      .format(w2v_dim, len(documents)))\n",
    "%time w2v = Word2Vec(documents, size=w2v_dim, window=5, min_count=10, sg=1, hs=0, workers=num_cores)\n",
    "print(\"Word2Vec {} model : {}\".format(suffix, w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the average feature vector for each document ...\n",
      "CPU times: user 12min 41s, sys: 24.5 s, total: 13min 5s\n",
      "Wall time: 13min 6s\n"
     ]
    }
   ],
   "source": [
    "# Get the average feature vector for each document (using the SG + NS model)\n",
    "print(\"Getting the average feature vector for each document ...\")\n",
    "%time doc_vecs = trainingutils.get_avg_feature_vectors(documents, w2v)\n",
    "\n",
    "# Handling missing values (if any)\n",
    "# (not doing this will give a \"ValueError: Input contains NaN, infinity\")\n",
    "doc_vecs = Imputer().fit_transform(doc_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model                                 F1-score\n",
      "----------------------------------  ----------\n",
      "PassiveAggressive_01, SG + NS           0.6875\n",
      "RandomForest_200, SG + NS               0.6757\n",
      "RidgeClassifier-auto-1e-3, SG + NS      0.6756\n",
      "RidgeClassifier-auto-1e-4, SG + NS      0.6756\n",
      "LogisticRegression, SG + NS             0.6754\n",
      "LinearSVC_01, SG + NS                   0.6751\n",
      "LinearSVC_02, SG + NS                   0.6750\n",
      "LinearSVC_07, SG + NS                   0.6750\n",
      "LinearSVC_10, SG + NS                   0.6750\n",
      "LinearSVC_04, SG + NS                   0.6750\n",
      "SGD_l1_penalty, SG + NS                 0.6650\n",
      "SGD_elasticnet_penalty, SG + NS         0.6649\n",
      "SGD_l2_penalty, SG + NS                 0.6646\n",
      "\n",
      "Time Taken : 10.6 minutes\n",
      "\n",
      "CPU times: user 32.3 s, sys: 13.5 s, total: 45.9 s\n",
      "Wall time: 10min 33s\n"
     ]
    }
   ],
   "source": [
    "# Train and compare a few classifiers based on the average feature vectors\n",
    "clfs = [\n",
    "    (\"LinearSVC_01, SG + NS\", LinearSVC(C=0.1, random_state=_random_state)),\n",
    "    (\"LinearSVC_02, SG + NS\", LinearSVC(C=0.2, random_state=_random_state)),\n",
    "    (\"LinearSVC_04, SG + NS\", LinearSVC(C=0.4, random_state=_random_state)),\n",
    "    (\"LinearSVC_07, SG + NS\", LinearSVC(C=0.7, random_state=_random_state)),\n",
    "    (\"LinearSVC_10, SG + NS\", LinearSVC(C=1.0, random_state=_random_state)),\n",
    "    (\"LogisticRegression, SG + NS\", LogisticRegression()),\n",
    "    (\"PassiveAggressive_01, SG + NS\", PassiveAggressiveClassifier(C=0.1, n_iter=50, random_state=_random_state)),\n",
    "    (\"RandomForest_200, SG + NS\", RandomForestClassifier(n_estimators = 200, random_state=_random_state)),\n",
    "    (\"RidgeClassifier-auto-1e-3, SG + NS\", RidgeClassifier(tol=1e-3, solver=\"auto\", random_state=_random_state)),\n",
    "    (\"RidgeClassifier-auto-1e-4, SG + NS\", RidgeClassifier(tol=1e-4, solver=\"auto\", random_state=_random_state)),\n",
    "    (\"SGD_elasticnet_penalty, SG + NS\", \n",
    "     SGDClassifier(alpha=.0001, n_iter=150, penalty=\"elasticnet\", random_state=_random_state)),\n",
    "    (\"SGD_l1_penalty, SG + NS\", SGDClassifier(alpha=.0001, n_iter=150, penalty=\"l1\", random_state=_random_state)),\n",
    "    (\"SGD_l2_penalty, SG + NS\", SGDClassifier(alpha=.0001, n_iter=150, penalty=\"l2\", random_state=_random_state)),\n",
    "]\n",
    "\n",
    "%time scores_w2v_sgns = trainingutils.compare_classifiers(clfs, doc_vecs, y, n_jobs=num_cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 Word2Vec - Skip-gram (SG) using hierarchical softmax (Using average feature vectors for each document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing 100-dimension Word2Vec model (Skip-gram using hierarchical softmax) based on text of 145044 blog posts\n",
      "CPU times: user 21min 42s, sys: 27.9 s, total: 22min 10s\n",
      "Wall time: 2min 54s\n",
      "Word2Vec SG + HS model : Word2Vec(vocab=41338, size=100, alpha=0.025)\n",
      "Getting the average feature vector for each document ...\n",
      "CPU times: user 12min 25s, sys: 26.2 s, total: 12min 52s\n",
      "Wall time: 12min 53s\n"
     ]
    }
   ],
   "source": [
    "# Construct the Word2Vec SG + HS model\n",
    "w2v_dim = 100\n",
    "suffix = \"SG + HS\"\n",
    "print(\"Constructing {}-dimension Word2Vec model (Skip-gram using hierarchical softmax) based on text of {} blog posts\"\\\n",
    "      .format(w2v_dim, len(documents)))\n",
    "%time w2v = Word2Vec(documents, size=w2v_dim, window=5, min_count=10, sg=1, hs=1, workers=num_cores)\n",
    "print(\"Word2Vec {} model : {}\".format(suffix, w2v))\n",
    "\n",
    "# Get the average feature vector for each document (using the SG + HS model)\n",
    "print(\"Getting the average feature vector for each document ...\")\n",
    "%time doc_vecs = trainingutils.get_avg_feature_vectors(documents, w2v)\n",
    "\n",
    "# Handling missing values (if any)\n",
    "# (not doing this will give a \"ValueError: Input contains NaN, infinity\")\n",
    "doc_vecs = Imputer().fit_transform(doc_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model                                 F1-score\n",
      "----------------------------------  ----------\n",
      "PassiveAggressive_01, SG + HS           0.6847\n",
      "RidgeClassifier-auto-1e-3, SG + HS      0.6746\n",
      "RidgeClassifier-auto-1e-4, SG + HS      0.6746\n",
      "LogisticRegression, SG + HS             0.6745\n",
      "LinearSVC_07, SG + HS                   0.6742\n",
      "LinearSVC_10, SG + HS                   0.6742\n",
      "LinearSVC_04, SG + HS                   0.6742\n",
      "LinearSVC_01, SG + HS                   0.6741\n",
      "LinearSVC_02, SG + HS                   0.6741\n",
      "RandomForest_200, SG + HS               0.6730\n",
      "SGD_l1_penalty, SG + HS                 0.6630\n",
      "SGD_l2_penalty, SG + HS                 0.6626\n",
      "SGD_elasticnet_penalty, SG + HS         0.6625\n",
      "\n",
      "Time Taken : 13.9 minutes\n",
      "\n",
      "CPU times: user 33.7 s, sys: 15.8 s, total: 49.4 s\n",
      "Wall time: 13min 53s\n"
     ]
    }
   ],
   "source": [
    "# Train and compare a few classifiers based on the average feature vectors\n",
    "clfs = [\n",
    "    (\"LinearSVC_01, SG + HS\", LinearSVC(C=0.1, random_state=_random_state)),\n",
    "    (\"LinearSVC_02, SG + HS\", LinearSVC(C=0.2, random_state=_random_state)),\n",
    "    (\"LinearSVC_04, SG + HS\", LinearSVC(C=0.4, random_state=_random_state)),\n",
    "    (\"LinearSVC_07, SG + HS\", LinearSVC(C=0.7, random_state=_random_state)),\n",
    "    (\"LinearSVC_10, SG + HS\", LinearSVC(C=1.0, random_state=_random_state)),\n",
    "    (\"LogisticRegression, SG + HS\", LogisticRegression()),\n",
    "    (\"PassiveAggressive_01, SG + HS\", PassiveAggressiveClassifier(C=0.1, n_iter=50, random_state=_random_state)),\n",
    "    (\"RandomForest_200, SG + HS\", RandomForestClassifier(n_estimators = 200, random_state=_random_state)),\n",
    "    (\"RidgeClassifier-auto-1e-3, SG + HS\", RidgeClassifier(tol=1e-3, solver=\"auto\", random_state=_random_state)),\n",
    "    (\"RidgeClassifier-auto-1e-4, SG + HS\", RidgeClassifier(tol=1e-4, solver=\"auto\", random_state=_random_state)),\n",
    "    (\"SGD_elasticnet_penalty, SG + HS\", \n",
    "     SGDClassifier(alpha=.0001, n_iter=150, penalty=\"elasticnet\", random_state=_random_state)),\n",
    "    (\"SGD_l1_penalty, SG + HS\", SGDClassifier(alpha=.0001, n_iter=150, penalty=\"l1\", random_state=_random_state)),\n",
    "    (\"SGD_l2_penalty, SG + HS\", SGDClassifier(alpha=.0001, n_iter=150, penalty=\"l2\", random_state=_random_state)),\n",
    "]\n",
    "\n",
    "%time scores_w2v_sghs = trainingutils.compare_classifiers(clfs, doc_vecs, y, n_jobs=num_cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Final Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "Model                                  F1-score\n",
      "-----------------------------------  ----------\n",
      "MultinomialNB_01, BOW, 20K               0.7307\n",
      "LinearSVC_10, BOW, 20K                   0.7296\n",
      "MultinomialNB_02, BOW, 20K               0.7294\n",
      "RidgeClassifier-auto-1e-3, BOW, 20K      0.7293\n",
      "RidgeClassifier-auto-1e-4, BOW, 20K      0.7293\n",
      "LinearSVC_07, BOW, 20K                   0.7289\n",
      "MultinomialNB_04, BOW, 20K               0.7277\n",
      "LinearSVC_04, BOW, 20K                   0.7276\n",
      "MultinomialNB_06, BOW, 20K               0.7258\n",
      "MultinomialNB_08, BOW, 20K               0.7249\n",
      "PassiveAggressive_01, BOW, 20K           0.7246\n",
      "MultinomialNB_10, BOW, 20K               0.7240\n",
      "MultinomialNB_01, BOW, 30K               0.7235\n",
      "LinearSVC_01, BOW, 20K                   0.7223\n",
      "MultinomialNB_15, BOW, 20K               0.7220\n",
      "RidgeClassifier-auto-1e-4, BOW, 30K      0.7218\n",
      "RidgeClassifier-auto-1e-3, BOW, 30K      0.7218\n",
      "LinearSVC_10, BOW, 30K                   0.7215\n",
      "SGD_l2_penalty, BOW, 20K                 0.7213\n",
      "LogisticRegression, BOW, 20K             0.7211\n",
      "MultinomialNB_02, BOW, 30K               0.7210\n",
      "LinearSVC_07, BOW, 30K                   0.7209\n",
      "LinearSVC_04, BOW, 30K                   0.7201\n",
      "MultinomialNB_04, BOW, 30K               0.7185\n",
      "SGD_l2_penalty, BOW, 30K                 0.7182\n",
      "PassiveAggressive_01, BOW, 30K           0.7173\n",
      "LinearSVC_01, BOW, 30K                   0.7168\n",
      "MultinomialNB_06, BOW, 30K               0.7168\n",
      "LogisticRegression, BOW, 30K             0.7155\n",
      "MultinomialNB_08, BOW, 30K               0.7152\n",
      "SGD_elasticnet_penalty, BOW, 20K         0.7150\n",
      "MultinomialNB_10, BOW, 30K               0.7145\n",
      "MultinomialNB_15, BOW, 30K               0.7129\n",
      "SGD_elasticnet_penalty, BOW, 30K         0.7128\n",
      "SGD_l1_penalty, BOW, 20K                 0.7074\n",
      "SGD_l1_penalty, BOW, 30K                 0.7067\n",
      "PassiveAggressive_01, SG + NS            0.6875\n",
      "PassiveAggressive_01, SG + HS            0.6847\n",
      "RandomForest_200, SG + NS                0.6757\n",
      "RidgeClassifier-auto-1e-3, SG + NS       0.6756\n",
      "RidgeClassifier-auto-1e-4, SG + NS       0.6756\n",
      "LogisticRegression, SG + NS              0.6754\n",
      "LinearSVC_01, SG + NS                    0.6751\n",
      "LinearSVC_02, SG + NS                    0.6750\n",
      "LinearSVC_07, SG + NS                    0.6750\n",
      "LinearSVC_10, SG + NS                    0.6750\n",
      "LinearSVC_04, SG + NS                    0.6750\n",
      "RidgeClassifier-auto-1e-3, SG + HS       0.6746\n",
      "RidgeClassifier-auto-1e-4, SG + HS       0.6746\n",
      "LogisticRegression, SG + HS              0.6745\n",
      "LinearSVC_07, SG + HS                    0.6742\n",
      "LinearSVC_10, SG + HS                    0.6742\n",
      "LinearSVC_04, SG + HS                    0.6742\n",
      "LinearSVC_01, SG + HS                    0.6741\n",
      "LinearSVC_02, SG + HS                    0.6741\n",
      "RandomForest_200, SG + HS                0.6730\n",
      "RidgeClassifier-auto-1e-3, CBOW          0.6718\n",
      "RidgeClassifier-auto-1e-4, CBOW          0.6718\n",
      "LinearSVC_10, CBOW                       0.6717\n",
      "LinearSVC_07, CBOW                       0.6716\n",
      "LinearSVC_04, CBOW                       0.6715\n",
      "LinearSVC_02, CBOW                       0.6714\n",
      "LinearSVC_01, CBOW                       0.6714\n",
      "LogisticRegression, CBOW                 0.6713\n",
      "RandomForest_200, CBOW                   0.6663\n",
      "SGD_l1_penalty, SG + NS                  0.6650\n",
      "SGD_elasticnet_penalty, SG + NS          0.6649\n",
      "SGD_l2_penalty, SG + NS                  0.6646\n",
      "SGD_l2_penalty, CBOW                     0.6641\n",
      "SGD_elasticnet_penalty, CBOW             0.6639\n",
      "SGD_l1_penalty, CBOW                     0.6636\n",
      "SGD_l1_penalty, SG + HS                  0.6630\n",
      "SGD_l2_penalty, SG + HS                  0.6626\n",
      "SGD_elasticnet_penalty, SG + HS          0.6625\n",
      "PassiveAggressive_01, CBOW               0.6123\n"
     ]
    }
   ],
   "source": [
    "scores = scores_bow + scores_w2v_cbow + scores_w2v_sgns + scores_w2v_sghs\n",
    "print(len(scores))\n",
    "scores = sorted(scores, key=lambda (_, x): -x)\n",
    "print(tabulate(scores, floatfmt=\".4f\", headers=(\"Model\", \"F1-score\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most BoW approaches score higher than Word2Vec approaches - was expecting otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
