{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify gender based on analysis of text - based on dataset containing 681288 blog posts downloaded from \n",
    "<a href=\"http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm\" target=\"_blank\">here</a>.\n",
    "\n",
    "I am using a **much smaller** dataset containing blog posts written by 3515 authors (1799 female, 1716 male) in the 24-25 age group.\n",
    "\n",
    "This is 2nd of 3 parts. In this notebook,\n",
    "1. First, I address some of the TODOs from previous part,\n",
    " - Some basic preprocessing on the text - helped improve scores\n",
    " - Fixed whatever caused the 577 errors during reading\n",
    " - Use **`GloVe`** word vector representation files - `small` and `medium` (400K words Vs. 1.9M words)\n",
    "2. Next, I build **`Word2Vec`** models on the blog text - both continuous bag-of-word (CBOW) and skip-gram (SG) models (hierarchical softmax and negative sampling)\n",
    "3. Next, I use a pretrained `Word2Vec` model on the Google News (100B) corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import parsing, utils\n",
    "\n",
    "import gc\n",
    "import itertools\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "import tarfile\n",
    "import time\n",
    "import traceback\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "def print_elapsed_time(ts=None):\n",
    "    if ts:\n",
    "        print \"\\nTime Taken :\", \"%.1f\" % ((time.time() - ts)/60), \"minutes\\n\"\n",
    "    else:\n",
    "        print \"\\nElapsed Time :\", \"%.1f\" % ((time.time() - t0)/60), \"minutes to reach this point (from the start)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading the dataset\n",
    "Each author's posts appear as a separate file. The name indicates blogger id#, self-provided gender, age, industry and astrological sign. (All are labeled for gender and age but for many, industry and/or sign is marked as unknown.)\n",
    "\n",
    "The work for reading the XML files from the `.tar.gz` file has been done by the script, `data_prep.py`.\n",
    "So, just resding the pre-created dataset and filtering out authors not in the 24-25 age group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3515, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>text_all_posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>female</td>\n",
       "      <td>and did i mention that i no longer have to dea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>male</td>\n",
       "      <td>B-Logs: The Business Blogs Paradox    urlLink ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>25</td>\n",
       "      <td>male</td>\n",
       "      <td>Planning the Marathon   I checked Active.com, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>25</td>\n",
       "      <td>female</td>\n",
       "      <td>MSN conversation: 11.17am   Iggbalbollywall  (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>24</td>\n",
       "      <td>female</td>\n",
       "      <td>You love me... I have you here by my side... O...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  gender                                     text_all_posts\n",
       "3    25  female  and did i mention that i no longer have to dea...\n",
       "4    25    male  B-Logs: The Business Blogs Paradox    urlLink ...\n",
       "8    25    male  Planning the Marathon   I checked Active.com, ...\n",
       "10   25  female  MSN conversation: 11.17am   Iggbalbollywall  (...\n",
       "17   24  female  You love me... I have you here by my side... O..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_filepath = \"blog_dataset_df\"\n",
    "an_iter = pd.read_csv(dataset_filepath, sep=\"\\t\", index_col=False,\n",
    "                      usecols=[\"age\", \"gender\", \"text_all_posts\"],\n",
    "                      iterator=True, chunksize=1000)\n",
    "df = pd.concat([chunk[chunk[\"age\"].isin([24,25])] for chunk in an_iter])\n",
    "print df.shape\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bag of Words (BoW) Approach\n",
    "based on the text in the `first_blog_post_preprocessed` column. Text tokenized to create an intermediate dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3515, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>text_all_posts</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>and did i mention that i no longer have to dea...</td>\n",
       "      <td>[and, did, i, mention, that, i, no, longer, ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>B-Logs: The Business Blogs Paradox    urlLink ...</td>\n",
       "      <td>[b-logs:, the, business, blogs, paradox, urlli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Planning the Marathon   I checked Active.com, ...</td>\n",
       "      <td>[planning, the, marathon, i, checked, active.c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>MSN conversation: 11.17am   Iggbalbollywall  (...</td>\n",
       "      <td>[msn, conversation:, 11.17am, iggbalbollywall,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>You love me... I have you here by my side... O...</td>\n",
       "      <td>[you, love, me..., i, have, you, here, by, my,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender                                     text_all_posts  \\\n",
       "0       2  and did i mention that i no longer have to dea...   \n",
       "1       1  B-Logs: The Business Blogs Paradox    urlLink ...   \n",
       "2       1  Planning the Marathon   I checked Active.com, ...   \n",
       "3       2  MSN conversation: 11.17am   Iggbalbollywall  (...   \n",
       "4       2  You love me... I have you here by my side... O...   \n",
       "\n",
       "                                      tokenized_text  \n",
       "0  [and, did, i, mention, that, i, no, longer, ha...  \n",
       "1  [b-logs:, the, business, blogs, paradox, urlli...  \n",
       "2  [planning, the, marathon, i, checked, active.c...  \n",
       "3  [msn, conversation:, 11.17am, iggbalbollywall,...  \n",
       "4  [you, love, me..., i, have, you, here, by, my,...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the 'age' column, not required for the BoW approach\n",
    "df.drop([\"age\"], axis=1, inplace=True)\n",
    "# Replace gender with a numeric value\n",
    "df[\"gender\"] = df[\"gender\"].replace({\"female\":2, \"male\":1})\n",
    "# Before tokenizing, drop rows where the text is NA\n",
    "df = df.dropna(subset=[\"text_all_posts\"])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print df.shape\n",
    "# Tokenize the text, prepare datset\n",
    "def my_func(x):\n",
    "    text = x[\"text_all_posts\"]\n",
    "    return text.lower().split() if text else []\n",
    "df[\"tokenized_text\"] = df.apply(my_func , axis=1)\n",
    "X, y = np.array(df.tokenized_text.values.tolist()), np.array(df.gender.values.tolist())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get an idea of the text of a blog post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you', 'love', 'me...', 'i', 'have', 'you', 'here', 'by', 'my', 'side...', 'our', 'hearts', 'overflow', 'with', 'happiness', 'and', 'love', 'for', 'each', 'other...', \"that's\", 'all', 'that', 'matters', 'to', 'me', 'now...', 'sorry', 'i', 'ever', 'doubted', 'you', 'and', 'your', 'love...', 'i', 'just', \"don't\", 'feel', 'too', 'special', 'anymore...', 'maybe', \"i'm\", 'not', 'the', 'one', 'you', 'should', 'love...', 'maybe', \"you're\", 'beside', 'the', 'wrong', 'person...', 'maybe', 'you', 'could', 'have', 'been', 'happier', 'with', 'her...', \"i'm\", 'not', 'that', 'special,', 'you', 'know...', '(as', 'if', 'on', 'cue,', 'i', 'hear', '\"our\"', 'songs', 'play', 'on', 'my', 'winamp)', 'what', 'the', 'fuck', 'am', 'i', 'saying?!?!?', 'you', 'love', 'me...', 'i', 'have', 'you', 'here', 'by', 'my', 'side...', 'our', 'hearts', 'overflow', 'with', 'happiness', 'and', 'love', 'for', 'each', 'other...', \"that's\", 'all', 'that', 'matters', 'to', 'me', 'now...', 'sorry', 'i', 'ever', 'doubted', 'you', 'and', 'your', 'love...', 'i', 'truly', 'am', 'sorry,', 'baby...', 'our', 'bestfriend', 'paranoia', 'paid', 'me', 'a', 'visit...', '\"...thanks', 'to', 'you', 'now', 'i', 'know', 'all', 'my', 'dreams', 'can', 'come', 'true...', '...and', \"i'm\", 'not', 'sure', 'i', 'deserve', 'a', 'woman', 'so', 'true', 'but', 'i', 'love', 'that', 'you', 'think', 'i', 'do...', \"...i've\", 'got', 'blind', 'faith', 'in', 'you...\"', 'you', 'sang', 'these', 'words', 'to', 'me', 'as', 'if', 'only', 'me', 'could', 'have', 'made', 'your', 'life', 'complete,', 'as', 'if', 'only', 'me', 'could', 'have', 'made', 'such', 'difference...', 'i', 'wonder', 'if', 'you', 'knew', 'that', 'you', 'are', 'the', 'one', 'who', 'turned', 'my', 'life', 'around', 'and', 'showed', 'me', 'who', 'i', 'really', 'am..', '...that', 'i', 'was', 'afraid', 'of', 'dreaming', 'and', 'never', 'really', 'believed', 'in', 'dreams,', 'until', 'you', 'came', 'and', 'showed', 'me', 'that', 'dreaming', 'can', 'be', 'beautiful', 'and', 'making', 'them', 'come', 'true', 'is', 'worth', 'all', 'the', 'pain', 'that', 'we', 'might', 'go', 'through...', '...that', 'with', 'you', 'by', 'my', 'side,', 'life', 'can', 'throw', 'her', 'worst', 'shit', 'at', 'me,', 'and', \"i'll\", 'just', 'show', 'her', 'my', 'finger', 'and', 'laugh', 'at', 'her', 'face...', '...that', 'you', 'literally', 'brought', 'back', 'the', 'real', 'me', 'from', 'a', 'world', 'where', 'showing', 'my', 'true', 'colors', 'cost', 'too', 'much...', '...that', 'with', 'your', '\"blind', 'faith,\"', 'i', 'see', 'more...', '...that', 'whatever', 'i', 'say', \"wouldn't\", 'amount', 'to', 'what', 'i', 'feel', 'for', 'you', 'and', 'how', 'you', 'make', 'me', 'feel...', 'i', 'wonder', 'if', 'you', 'know', 'how', 'much', 'i', 'love', 'you...', 'http://www.kingsofchaos.com/recruit.php?uniqid=jm8bja2z', 'my', 'first', 'blog...', 'here', 'at', 'my', 'workplace..', 'got', 'lots', 'to', 'do,', 'but', \"don't\", 'know', 'where', 'to', 'start...', 'my', \"mind's\", 'gone', 'blank...', 'need', 'some', 'coffee...']\n"
     ]
    }
   ],
   "source": [
    "print df.loc[4].tokenized_text[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the text\n",
    "- Strip off **repeating characters** from words. For example,\n",
    " - *AAAAAAAAAAARGH* becomes *AARGH*\n",
    " - *AAAAAAAAARRRRRRRGGGGGGGGGHHHHHHHHHHHH* becomes *AARRGGHH*\n",
    " - *daaaaarling* becomes *daarling*\n",
    " - *yeaaaaar* becomes *yeaar*\n",
    "- expand contractions (\"*i'm*\" becomes \"*i am*\")\n",
    "- remove a subset of punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading English contractions from english_contractions.json\n"
     ]
    }
   ],
   "source": [
    "def expand_contraction(token):\n",
    "    # if not isinstance(token, unicode):\n",
    "    #    token = unicode(token)\n",
    "    return english_contractions[token] if token in english_contractions else token\n",
    "\n",
    "def load_english_contractions(file_path):\n",
    "    english_contractions = {}\n",
    "    if file_path is not None:\n",
    "        try:\n",
    "            print(\"Reading English contractions from %s\" % file_path)\n",
    "            english_contractions = json.load(open(file_path))\n",
    "        except Exception, e:\n",
    "            print(str(e))\n",
    "            pass\n",
    "    return english_contractions\n",
    "\n",
    "def preprocess_token(token):\n",
    "    corrected_token = remove_repeated_chars(token)\n",
    "    corrected_token = expand_contraction(corrected_token)\n",
    "    return unicode(remove_punct(corrected_token))\n",
    "\n",
    "def remove_punct(token):\n",
    "    #return token.rstrip(\".?:!,*\")\n",
    "    return re.sub(r\"\\W+\", \" \", token).strip()\n",
    "\n",
    "def remove_repeated_chars(token):\n",
    "    # Credit : http://stackoverflow.com/a/10072826\n",
    "    return re.sub(r'(.)\\1+', r'\\1\\1', token)\n",
    "\n",
    "english_contractions = load_english_contractions(\"english_contractions.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a simple test - combining the preprocessing steps above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAAAAAAAAAARGH --> AARGH --> AARGH --> AARGH\n",
      "AAAAAAAAARRRRRRRGGGGGGGGGHHHHHHHHHHHH --> AARRGGHH --> AARRGGHH --> AARRGGHH\n",
      "(as --> (as --> (as --> as\n",
      "b-logs: --> b-logs: --> b-logs: --> b logs\n",
      "daaaaarling --> daarling --> daarling --> daarling\n",
      "person... --> person.. --> person.. --> person\n",
      "saying?!?!? --> saying?!?!? --> saying?!?!? --> saying\n",
      "that's --> that's --> that is --> that is\n",
      "yeaaaaar --> yeaar --> yeaar --> yeaar\n"
     ]
    }
   ],
   "source": [
    "for w in [\"AAAAAAAAAAARGH\", \"AAAAAAAAARRRRRRRGGGGGGGGGHHHHHHHHHHHH\", \"(as\", \"b-logs:\", \"daaaaarling\", \n",
    "          \"person...\", \"saying?!?!?\", \"that's\", \"yeaaaaar\"]:\n",
    "    print w, \"-->\", remove_repeated_chars(w), \"-->\", expand_contraction(remove_repeated_chars(w)), \"-->\",\\\n",
    "            remove_punct(expand_contraction(remove_repeated_chars(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run the preprocessing steps on 'tokenized_text' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dimensions (distinct tokens) : 869687 (before preprocessing)\n",
      "Number of dimensions (distinct tokens) : 428162 (after preprocessing)\n",
      "869687\n",
      "\n",
      "Time Taken : 0.5 minutes\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>text_all_posts</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>and did i mention that i no longer have to dea...</td>\n",
       "      <td>[and, did, i, mention, that, i, no, longer, ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>B-Logs: The Business Blogs Paradox    urlLink ...</td>\n",
       "      <td>[b logs, the, business, blogs, paradox, urllin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Planning the Marathon   I checked Active.com, ...</td>\n",
       "      <td>[planning, the, marathon, i, checked, active c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>MSN conversation: 11.17am   Iggbalbollywall  (...</td>\n",
       "      <td>[msn, conversation, 11 17am, iggbalbollywall, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>You love me... I have you here by my side... O...</td>\n",
       "      <td>[you, love, me, i, have, you, here, by, my, si...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender                                     text_all_posts  \\\n",
       "0       2  and did i mention that i no longer have to dea...   \n",
       "1       1  B-Logs: The Business Blogs Paradox    urlLink ...   \n",
       "2       1  Planning the Marathon   I checked Active.com, ...   \n",
       "3       2  MSN conversation: 11.17am   Iggbalbollywall  (...   \n",
       "4       2  You love me... I have you here by my side... O...   \n",
       "\n",
       "                                      tokenized_text  \n",
       "0  [and, did, i, mention, that, i, no, longer, ha...  \n",
       "1  [b logs, the, business, blogs, paradox, urllin...  \n",
       "2  [planning, the, marathon, i, checked, active c...  \n",
       "3  [msn, conversation, 11 17am, iggbalbollywall, ...  \n",
       "4  [you, love, me, i, have, you, here, by, my, si...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the text, prepare datset\n",
    "ts = time.time()\n",
    "\n",
    "# Prepare a look up dictionary between token and its corrected version\n",
    "features = set(list(itertools.chain(*df['tokenized_text'].values.tolist())))\n",
    "# Maintain a mapping between original token and correction\n",
    "corrected = dict(zip(features, [None]*len(features)))\n",
    "print \"Number of dimensions (distinct tokens) :\", len(features), \"(before preprocessing)\" # 89487\n",
    "\n",
    "# Preprocess each token (remove subset of punctuations, expand contractions, etc.)\n",
    "# CANT Do the spell checks - because the current implementation is very slow\n",
    "for token in corrected.keys():\n",
    "    corrected[token] = preprocess_token(token)\n",
    "\n",
    "def preprocess_tokens(x):\n",
    "    text = x[\"tokenized_text\"]\n",
    "    output = []\n",
    "    if text:\n",
    "        for token in text:\n",
    "            corrected_token = None\n",
    "            if token in corrected:\n",
    "                corrected_token = corrected[token]\n",
    "            else:\n",
    "                corrected[token] = preprocess_token(token)\n",
    "            if len(corrected_token) > 0:\n",
    "                output.append(corrected_token)\n",
    "    return output\n",
    "\n",
    "df[\"tokenized_text\"] = df.apply(preprocess_tokens, axis=1)\n",
    "features = set(list(itertools.chain(*df['tokenized_text'].values.tolist())))\n",
    "print \"Number of dimensions (distinct tokens) :\", len(features), \"(after preprocessing)\" # 51396\n",
    "X, y = np.array(df.tokenized_text.values.tolist()), np.array(df.gender.values.tolist())\n",
    "print len(corrected.keys())\n",
    "del corrected\n",
    "print_elapsed_time(ts)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train basic models\n",
    "- Experiment with the vectorizers - each will give different number of features.\n",
    "- Next, run grid search to pick the best hyperparameters for some of the commonly used classifiers (RF, SVM, etc.)\n",
    "- Next, train the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18471 features for minimum document frequency 1.0%\n",
      "top 8 terms [u'i' u'and' u'my' u'to' u'the' u'a' u'of' u'it'] \n",
      "\n",
      "28284 features for minimum document frequency 0.5%\n",
      "top 8 terms [u'i' u'and' u'my' u'to' u'the' u'a' u'of' u'it'] \n",
      "\n",
      "\n",
      "Time Taken : 0.6 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Experiment with the vectorizers\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "ts = time.time()\n",
    "min_dfs = [0.01, 0.005]\n",
    "tfidf_vec = [None] * len(min_dfs)\n",
    "features = [None] * len(min_dfs)\n",
    "ctr = 0\n",
    "\n",
    "def get_topn_tfidf_vec_terms(vec, features, n=5):\n",
    "    # Get the top n terms with highest tf-idf score\n",
    "    # Credit : http://stackoverflow.com/a/34236002\n",
    "    feature_array = np.array(vec.get_feature_names())\n",
    "    tfidf_sorting = np.argsort(features.toarray()).flatten()[::-1]\n",
    "    return feature_array[tfidf_sorting][:n]\n",
    "\n",
    "for min_df in min_dfs:\n",
    "    tfidf_vec[ctr] = TfidfVectorizer(analyzer=lambda x: x, min_df=min_df)\n",
    "    features[ctr] = tfidf_vec[ctr].fit_transform(X)\n",
    "    print features[ctr].shape[1], \\\n",
    "          \"features for minimum document frequency %.1f%%\\n\" % (min_df * 100), \\\n",
    "          \"top 8 terms\", get_topn_tfidf_vec_terms(tfidf_vec[ctr], features[ctr], n=8), \"\\n\"\n",
    "    ctr += 1\n",
    "\n",
    "print_elapsed_time(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding best hyperparameters for RandomForestClassifier\n",
      "Model with rank: 1\n",
      "\tMean validation score: 0.742 (std: 0.015)\n",
      "\tParameters: {'n_estimators': 300, 'bootstrap': False, 'criterion': 'entropy', 'max_depth': 9}\n",
      "Model with rank: 2\n",
      "\tMean validation score: 0.737 (std: 0.009)\n",
      "\tParameters: {'n_estimators': 450, 'bootstrap': False, 'criterion': 'gini', 'max_depth': 9}\n",
      "Model with rank: 3\n",
      "\tMean validation score: 0.737 (std: 0.015)\n",
      "\tParameters: {'n_estimators': 250, 'bootstrap': False, 'criterion': 'entropy', 'max_depth': 9}\n",
      "RandomForestClassifier(bootstrap=False, class_weight=None,\n",
      "            criterion='entropy', max_depth=9, max_features='auto',\n",
      "            max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=1,\n",
      "            oob_score=False, random_state=120, verbose=0, warm_start=False) \n",
      "\n",
      "Finding best hyperparameters for ExtraTreesClassifier\n",
      "Model with rank: 1\n",
      "\tMean validation score: 0.742 (std: 0.021)\n",
      "\tParameters: {'n_estimators': 450, 'max_depth': None}\n",
      "Model with rank: 2\n",
      "\tMean validation score: 0.736 (std: 0.010)\n",
      "\tParameters: {'n_estimators': 150, 'max_depth': None}\n",
      "Model with rank: 3\n",
      "\tMean validation score: 0.733 (std: 0.011)\n",
      "\tParameters: {'n_estimators': 200, 'max_depth': None}\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=450, n_jobs=1,\n",
      "           oob_score=False, random_state=9000, verbose=0, warm_start=False) \n",
      "\n",
      "Finding best hyperparameters for SVC\n",
      "Model with rank: 1\n",
      "\tMean validation score: 0.751 (std: 0.022)\n",
      "\tParameters: {'C': 0.9, 'gamma': 0.2, 'tol': 0.01}\n",
      "Model with rank: 2\n",
      "\tMean validation score: 0.751 (std: 0.022)\n",
      "\tParameters: {'C': 0.9, 'gamma': 0.5, 'tol': 0.001}\n",
      "Model with rank: 3\n",
      "\tMean validation score: 0.751 (std: 0.022)\n",
      "\tParameters: {'C': 0.9, 'gamma': 0.1, 'tol': 0.0001}\n",
      "SVC(C=0.9, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.2, kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=840, shrinking=True,\n",
      "  tol=0.01, verbose=False) \n",
      "\n",
      "Finding best hyperparameters for LinearSVC\n",
      "Model with rank: 1\n",
      "\tMean validation score: 0.776 (std: 0.020)\n",
      "\tParameters: {'loss': 'squared_hinge', 'C': 0.8, 'tol': 0.0001}\n",
      "Model with rank: 2\n",
      "\tMean validation score: 0.774 (std: 0.019)\n",
      "\tParameters: {'loss': 'squared_hinge', 'C': 0.7000000000000001, 'tol': 0.001}\n",
      "Model with rank: 3\n",
      "\tMean validation score: 0.767 (std: 0.018)\n",
      "\tParameters: {'loss': 'squared_hinge', 'C': 0.5, 'tol': 0.01}\n",
      "LinearSVC(C=0.8, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=11640, tol=0.0001,\n",
      "     verbose=0) \n",
      "\n",
      "Finding best hyperparameters for SVC\n",
      "Model with rank: 1\n",
      "\tMean validation score: 0.732 (std: 0.018)\n",
      "\tParameters: {'C': 0.8, 'gamma': 0.5, 'tol': 0.0001}\n",
      "Model with rank: 2\n",
      "\tMean validation score: 0.729 (std: 0.018)\n",
      "\tParameters: {'C': 0.8, 'gamma': 0.4, 'tol': 0.001}\n",
      "Model with rank: 3\n",
      "\tMean validation score: 0.725 (std: 0.017)\n",
      "\tParameters: {'C': 0.7000000000000001, 'gamma': 0.4, 'tol': 0.01}\n",
      "SVC(C=0.8, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.5, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=600, shrinking=True,\n",
      "  tol=0.0001, verbose=False)\n",
      "\n",
      "Time Taken : 32.1 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Next, run grid search to pick the best hyperparameters \n",
    "from operator import itemgetter\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "def find_best_hyperparameters(clf, vectorizer, param_dist, num_iters=20):\n",
    "    # Run the grid search\n",
    "    print \"Finding best hyperparameters for\", clf.__class__.__name__\n",
    "    random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
    "                                       n_iter=num_iters, n_jobs=7)\n",
    "    random_search.fit(vectorizer.fit_transform(X), y)\n",
    "    # Iterate through the scores and print the best 3\n",
    "    top_scores = sorted(random_search.grid_scores_, key=itemgetter(1), reverse=True)[:3]\n",
    "    for i, score in enumerate(top_scores):\n",
    "        print(\"Model with rank: {0}\".format(i + 1))\n",
    "        print(\"\\tMean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "              score.mean_validation_score,\n",
    "              np.std(score.cv_validation_scores)))\n",
    "        print(\"\\tParameters: {0}\".format(score.parameters))\n",
    "    # print top_scores[0]\n",
    "    return random_search.best_estimator_\n",
    "\n",
    "# Using the TfidfVectorizer for minimum document frequency 0.5%\n",
    "ts = time.time()\n",
    "best_rf = find_best_hyperparameters(RandomForestClassifier(random_state = 120), tfidf_vec[1],\n",
    "                                    { \"bootstrap\": [True, False],\n",
    "                                      \"criterion\": [\"gini\", \"entropy\"],\n",
    "                                      \"max_depth\": np.arange(5, 11).tolist() + [None],\n",
    "                                      \"n_estimators\": np.arange(50, 550, 50).tolist()\n",
    "                                    },\n",
    "                                    num_iters=20)\n",
    "print best_rf,\"\\n\"\n",
    "\n",
    "best_et = find_best_hyperparameters(ExtraTreesClassifier(random_state = 9000), tfidf_vec[1],\n",
    "                                    { \"max_depth\": np.arange(5, 11).tolist() + [None],\n",
    "                                      \"n_estimators\": np.arange(50, 550, 50).tolist(),\n",
    "                                    },\n",
    "                                    num_iters=20)\n",
    "print best_et,\"\\n\"\n",
    "\n",
    "best_svm = find_best_hyperparameters(SVC(kernel=\"linear\", random_state = 840), tfidf_vec[1],\n",
    "                                    { \"C\" : np.arange(0.1, 1, 0.1).tolist(),\n",
    "                                      \"gamma\": [0.1, 0.2, 0.3, 0.4, 0.5, \"auto\"],\n",
    "                                      \"tol\": [0.0001, 0.001, 0.01]\n",
    "                                    },\n",
    "                                    num_iters=20)\n",
    "print best_svm,\"\\n\"\n",
    "\n",
    "best_linearsvc = find_best_hyperparameters(LinearSVC(random_state = 11640), tfidf_vec[1],\n",
    "                                           { \"C\" : np.arange(0.1, 1, 0.1).tolist(),\n",
    "                                             \"loss\": [\"hinge\", \"squared_hinge\"],\n",
    "                                             \"tol\": [0.0001, 0.001, 0.01]\n",
    "                                           },\n",
    "                                           num_iters=20)\n",
    "print best_linearsvc,\"\\n\"\n",
    "\n",
    "best_svm_rbf = find_best_hyperparameters(SVC(kernel=\"rbf\", random_state = 600), tfidf_vec[1],\n",
    "                                         { \"C\" : np.arange(0.1, 1, 0.1).tolist(),\n",
    "                                           \"gamma\": [0.1, 0.2, 0.3, 0.4, 0.5, \"auto\"],\n",
    "                                           \"tol\": [0.0001, 0.001, 0.01]\n",
    "                                         },\n",
    "                                         num_iters=20)\n",
    "print best_svm_rbf\n",
    "print_elapsed_time(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 3 Nearest Neighbors, TF-IDF, min_df 0.5%\n",
      "\tScore: 0.6270\n",
      "\tTime taken: 1.5 minutes\n",
      "\n",
      "Training: 5 Nearest Neighbors, TF-IDF, min_df 0.5%\n",
      "\tScore: 0.6461\n",
      "\tTime taken: 1.6 minutes\n",
      "\n",
      "Training: SVM (Linear), TF-IDF, min_df 0.5%\n",
      "\tScore: 0.7562\n",
      "\tTime taken: 7.7 minutes\n",
      "\n",
      "Training: Extra Trees, TF-IDF, min_df 0.5%\n",
      "\tScore: 0.7439\n",
      "\tTime taken: 7.6 minutes\n",
      "\n",
      "Training: Random Forest, TF-IDF, min_df 0.5%\n",
      "\tScore: 0.7388\n",
      "\tTime taken: 3.6 minutes\n",
      "\n",
      "Training: LinearSVC, TF-IDF, min_df 0.5%\n",
      "\tScore: 0.7801\n",
      "\tTime taken: 2.3 minutes\n",
      "\n",
      "Training: MultinomialNB, TF-IDF, min_df 1.0%\n",
      "\tScore: 0.6529\n",
      "\tTime taken: 1.5 minutes\n",
      "\n",
      "Training: MultinomialNB, TF-IDF, min_df 0.5%\n",
      "\tScore: 0.6452\n",
      "\tTime taken: 1.2 minutes\n",
      "\n",
      "Training: SVM (RBF), TF-IDF, min_df 0.5%\n",
      "\tScore: 0.7408\n",
      "\tTime taken: 7.4 minutes\n",
      "\n",
      "model                                       score\n",
      "----------------------------------------  -------\n",
      "LinearSVC, TF-IDF, min_df 0.5%             0.7801\n",
      "SVM (Linear), TF-IDF, min_df 0.5%          0.7562\n",
      "Extra Trees, TF-IDF, min_df 0.5%           0.7439\n",
      "SVM (RBF), TF-IDF, min_df 0.5%             0.7408\n",
      "Random Forest, TF-IDF, min_df 0.5%         0.7388\n",
      "MultinomialNB, TF-IDF, min_df 1.0%         0.6529\n",
      "5 Nearest Neighbors, TF-IDF, min_df 0.5%   0.6461\n",
      "MultinomialNB, TF-IDF, min_df 0.5%         0.6452\n",
      "3 Nearest Neighbors, TF-IDF, min_df 0.5%   0.6270\n",
      "\n",
      "Time Taken : 34.4 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Next, train the models\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tabulate import tabulate\n",
    "\n",
    "def get_cv_scores(models_with_desc):\n",
    "    cv_scores = []\n",
    "    for model_id, model in models_with_desc:\n",
    "        print \"Training:\", model_id\n",
    "        ts = time.time()\n",
    "        cv_score = cross_val_score(model, X, y, cv=5).mean() # gives a pickling error\n",
    "        # cv_score = cross_val_score(model, X, y, cv=5).mean()\n",
    "        cv_scores.append((model_id, cv_score))\n",
    "        print \"\\tScore:\", \"%.4f\" % cv_score\n",
    "        print \"\\tTime taken:\", \"%.1f\" % ((time.time() - ts)/60), \"minutes\\n\"\n",
    "    return cv_scores\n",
    "\n",
    "ts = time.time()\n",
    "models_with_desc = [\n",
    "    (\"3 Nearest Neighbors, TF-IDF, min_df 0.5%\", Pipeline([(\"tfidf_vec\", tfidf_vec[1]), (\"3nn\", KNeighborsClassifier(3))])),\n",
    "    (\"5 Nearest Neighbors, TF-IDF, min_df 0.5%\", Pipeline([(\"tfidf_vec\", tfidf_vec[1]), (\"5nn\", KNeighborsClassifier(5))])),\n",
    "    (\"SVM (Linear), TF-IDF, min_df 0.5%\", Pipeline([(\"tfidf_vec\", tfidf_vec[1]), (\"best_svm_linear\", best_svm)])),\n",
    "    (\"Extra Trees, TF-IDF, min_df 0.5%\", Pipeline([(\"tfidf_vec\", tfidf_vec[1]), (\"best_et\", best_et)])),\n",
    "    (\"Random Forest, TF-IDF, min_df 0.5%\", Pipeline([(\"tfidf_vec\", tfidf_vec[1]), (\"best_rf\", best_rf)])),\n",
    "    (\"LinearSVC, TF-IDF, min_df 0.5%\", Pipeline([(\"tfidf_vec\", tfidf_vec[1]), (\"best_linearsvc\", best_linearsvc)])),\n",
    "    (\"MultinomialNB, TF-IDF, min_df 1.0%\", Pipeline([(\"tfidf_vec\", tfidf_vec[0]), (\"mnb\", MultinomialNB())])),\n",
    "    (\"MultinomialNB, TF-IDF, min_df 0.5%\", Pipeline([(\"tfidf_vec\", tfidf_vec[1]), (\"mnb\", MultinomialNB())])),\n",
    "    (\"SVM (RBF), TF-IDF, min_df 0.5%\", Pipeline([(\"tfidf_vec\", tfidf_vec[1]), (\"best_svm_rbf\", best_svm_rbf)]))\n",
    "]\n",
    "\n",
    "scores = get_cv_scores(models_with_desc)\n",
    "scores = sorted(scores, key=lambda (_, x): -x)\n",
    "print tabulate(scores, floatfmt=\".4f\", headers=(\"model\", 'score'))\n",
    "print_elapsed_time(ts)\n",
    "\n",
    "del models_with_desc\n",
    "del tfidf_vec\n",
    "num_unreachable_objects = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using Word Vectors\n",
    "\n",
    "\n",
    "### 3.1 Using `GloVe` word vector representation files\n",
    "First, using **`GloVe`** word vector representation files downloaded from http://nlp.stanford.edu/data/ or https://github.com/stanfordnlp/GloVe. There are 3 files : \n",
    "- `glove.6B.zip` (6 Billion tokens, hence '*small*', 400K words) - has 50/100/200/300-dimension vectors,\n",
    "- `glove.42B.300d.zip` (42 Billion tokens, hence '*medium*', 1.9M words),\t\n",
    "- `glove.840B.300d.zip` (840 Billion tokens, hence '*large*', 2.2M words)\n",
    "\n",
    "Reference : http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the GloVe word vector representation file,\n",
    "# Using just the 300-dimension because glove_medium and glove_large only have 300-dimension vectors\n",
    "import gzip\n",
    "\n",
    "def read_GloVe_file(filepath):\n",
    "    print \"Reading\", filepath\n",
    "    glove_w2v = {}\n",
    "    with gzip.open(filepath, \"rb\") as lines:\n",
    "        for line in lines:\n",
    "            parts = line.split()\n",
    "            glove_w2v[parts[0]] = np.array(map(float, parts[1:]))\n",
    "    print len(glove_w2v.keys()), \"keys. First 8 :\", glove_w2v.keys()[:8], \"\\n\"\n",
    "    return glove_w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each word in each blog post needs to be mapped to its vector representation - which is accordingly used as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Word vector equivalent of CountVectorizer & TfidfVectorizer (respectively)\n",
    "# Each word in each blog post is mapped to its vector; \n",
    "# then this helper class computes the mean of those vectors\n",
    "# Credit : https://github.com/nadbordrozd/blog_stuff/blob/master/classification_w2v/benchmarking.ipynb\n",
    "from collections import defaultdict\n",
    "\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = len(word2vec.itervalues().next())\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        return self \n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec] \n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "\n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = len(word2vec.itervalues().next())\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of \n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf, \n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "    \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1  **`GloVe` *small***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the models based on the `GloVe` word vector representation files.\n",
    "The `TfidfEmbeddingVectorizer` is slower then the `MeanEmbeddingVectorizer`, so using the latter, first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading glove.6B.300d.txt.gz\n",
      "400000 keys. First 8 : ['biennials', 'verplank', 'soestdijk', 'woode', 'mdbo', 'sowell', 'mdbu', 'woods'] \n",
      "\n",
      "\n",
      "Time Taken : 0.8 minutes\n",
      "\n",
      "friday \tfirst 10 (of 300)\t[ 0.20283  -0.22845  -0.33061   0.25058   0.047943 -0.16453  -0.084213\n",
      " -0.16797   0.1062   -1.3609  ] ..\n",
      "night \tfirst 10 (of 300)\t[ 0.16882   0.041931 -0.068774  0.516    -0.40985   0.34697  -0.006856\n",
      "  0.080555 -0.091977 -0.56115 ] ..\n",
      "music \tfirst 10 (of 300)\t[-0.38081   -0.24764   -0.24949    0.10468   -0.56411   -0.80654   -0.057066\n",
      " -0.095754   0.0068887 -0.7162   ] ..\n",
      "Training: Random Forest, MeanEmbeddingVectorizer, GloVe small 300-Dim\n",
      "\tScore: 0.7223\n",
      "\tTime taken: 6.8 minutes\n",
      "\n",
      "Training: SVM (Linear), MeanEmbeddingVectorizer, GloVe small 300-Dim\n",
      "\tScore: 0.7476\n",
      "\tTime taken: 5.4 minutes\n",
      "\n",
      "Training: LinearSVC, MeanEmbeddingVectorizer, GloVe small 300-Dim\n",
      "\tScore: 0.7570\n",
      "\tTime taken: 5.5 minutes\n",
      "\n",
      "model                                                          score\n",
      "-----------------------------------------------------------  -------\n",
      "LinearSVC, TF-IDF, min_df 0.5%                                0.7801\n",
      "LinearSVC, MeanEmbeddingVectorizer, GloVe small 300-Dim       0.7570\n",
      "SVM (Linear), TF-IDF, min_df 0.5%                             0.7562\n",
      "SVM (Linear), MeanEmbeddingVectorizer, GloVe small 300-Dim    0.7476\n",
      "Extra Trees, TF-IDF, min_df 0.5%                              0.7439\n",
      "SVM (RBF), TF-IDF, min_df 0.5%                                0.7408\n",
      "Random Forest, TF-IDF, min_df 0.5%                            0.7388\n",
      "Random Forest, MeanEmbeddingVectorizer, GloVe small 300-Dim   0.7223\n",
      "MultinomialNB, TF-IDF, min_df 1.0%                            0.6529\n",
      "5 Nearest Neighbors, TF-IDF, min_df 0.5%                      0.6461\n",
      "MultinomialNB, TF-IDF, min_df 0.5%                            0.6452\n",
      "3 Nearest Neighbors, TF-IDF, min_df 0.5%                      0.6270\n",
      "\n",
      "Time Taken : 17.7 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ts = time.time()\n",
    "glove_300_w2v = read_GloVe_file(\"glove.6B.300d.txt.gz\")\n",
    "print_elapsed_time(ts)\n",
    "\n",
    "# Test whether some of the words are present in the GloVe word vector representation file\n",
    "for word in [\"friday\", \"night\", \"music\"]:\n",
    "    if word in glove_300_w2v:\n",
    "        print word, \"\\t\", \"first 10 (of 300)\\t\", glove_300_w2v[word][:10], \"..\"\n",
    "\n",
    "# Train\n",
    "ts = time.time()\n",
    "vec = MeanEmbeddingVectorizer(glove_300_w2v)\n",
    "\n",
    "models_with_desc = [\n",
    "    (\"Random Forest, MeanEmbeddingVectorizer, GloVe small 300-Dim\", Pipeline([(\"vec\", vec), (\"best_rf\", best_rf)])),\n",
    "    (\"SVM (Linear), MeanEmbeddingVectorizer, GloVe small 300-Dim\", Pipeline([(\"vec\", vec), (\"best_svm_linear\", best_svm)])),\n",
    "    (\"LinearSVC, MeanEmbeddingVectorizer, GloVe small 300-Dim\", Pipeline([(\"vec\", vec), (\"best_linearsvc\", best_linearsvc)]))\n",
    "    #(\"SVM (RBF) - 'Best', TF-IDF GloVe small 300-Dim\", Pipeline([(\"vec\", vec), (\"best_svm_rbf\", best_svm_rbf)]))\n",
    "]\n",
    "\n",
    "# scores = [] # Because we want to compare with the previous approaches\n",
    "scores.extend(get_cv_scores(models_with_desc))\n",
    "scores = sorted(scores, key=lambda (_, x): -x)\n",
    "print tabulate(scores, floatfmt=\".4f\", headers=(\"model\", 'score'))\n",
    "\n",
    "del models_with_desc\n",
    "num_unreachable_objects = gc.collect()\n",
    "\n",
    "print_elapsed_time(ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2  **`GloVe` *medium***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading glove.42B.300d.txt.gz\n",
      "1917495 keys. First 8 : ['ftdna', 'tripolitan', 'soestdijk', '6-night', 'un-loveyou', '20:09:49', '20:09:48', 'homespice'] \n",
      "\n",
      "\n",
      "Time Taken : 4.4 minutes\n",
      "\n",
      "Training: Random Forest, MeanEmbeddingVectorizer, GloVe medium 300-Dim\n",
      "\tScore: 0.7394\n",
      "\tTime taken: 10.9 minutes\n",
      "\n",
      "Training: SVM (Linear), MeanEmbeddingVectorizer, GloVe medium 300-Dim\n",
      "\tScore: 0.7604\n",
      "\tTime taken: 9.4 minutes\n",
      "\n",
      "Training: LinearSVC, MeanEmbeddingVectorizer, GloVe medium 300-Dim\n",
      "\tScore: 0.7744\n",
      "\tTime taken: 10.2 minutes\n",
      "\n",
      "model                                                           score\n",
      "------------------------------------------------------------  -------\n",
      "LinearSVC, TF-IDF, min_df 0.5%                                 0.7801\n",
      "LinearSVC, MeanEmbeddingVectorizer, GloVe medium 300-Dim       0.7744\n",
      "SVM (Linear), MeanEmbeddingVectorizer, GloVe medium 300-Dim    0.7604\n",
      "LinearSVC, MeanEmbeddingVectorizer, GloVe small 300-Dim        0.7570\n",
      "SVM (Linear), TF-IDF, min_df 0.5%                              0.7562\n",
      "SVM (Linear), MeanEmbeddingVectorizer, GloVe small 300-Dim     0.7476\n",
      "Extra Trees, TF-IDF, min_df 0.5%                               0.7439\n",
      "SVM (RBF), TF-IDF, min_df 0.5%                                 0.7408\n",
      "Random Forest, MeanEmbeddingVectorizer, GloVe medium 300-Dim   0.7394\n",
      "Random Forest, TF-IDF, min_df 0.5%                             0.7388\n",
      "Random Forest, MeanEmbeddingVectorizer, GloVe small 300-Dim    0.7223\n",
      "MultinomialNB, TF-IDF, min_df 1.0%                             0.6529\n",
      "5 Nearest Neighbors, TF-IDF, min_df 0.5%                       0.6461\n",
      "MultinomialNB, TF-IDF, min_df 0.5%                             0.6452\n",
      "3 Nearest Neighbors, TF-IDF, min_df 0.5%                       0.6270\n",
      "\n",
      "Time Taken : 30.6 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ts = time.time()\n",
    "glove_300_w2v = read_GloVe_file(\"glove.42B.300d.txt.gz\")\n",
    "print_elapsed_time(ts)\n",
    "\n",
    "ts = time.time()\n",
    "vec = MeanEmbeddingVectorizer(glove_300_w2v)\n",
    "\n",
    "models_with_desc = [\n",
    "    (\"Random Forest, MeanEmbeddingVectorizer, GloVe medium 300-Dim\", Pipeline([(\"vec\", vec), (\"best_rf\", best_rf)])),\n",
    "    (\"SVM (Linear), MeanEmbeddingVectorizer, GloVe medium 300-Dim\", Pipeline([(\"vec\", vec), (\"best_svm_linear\", best_svm)])),\n",
    "    (\"LinearSVC, MeanEmbeddingVectorizer, GloVe medium 300-Dim\", Pipeline([(\"vec\", vec), (\"best_linearsvc\", best_linearsvc)]))\n",
    "    #(\"SVM (RBF) - 'Best', TF-IDF GloVe medium 300-Dim\", Pipeline([(\"vec\", vec), (\"best_svm_rbf\", best_svm_rbf)]))\n",
    "]\n",
    "\n",
    "# scores = [] # Because we want to compare with the previous approaches\n",
    "scores.extend(get_cv_scores(models_with_desc))\n",
    "scores = sorted(scores, key=lambda (_, x): -x)\n",
    "print tabulate(scores, floatfmt=\".4f\", headers=(\"model\", 'score'))\n",
    "\n",
    "del models_with_desc\n",
    "num_unreachable_objects = gc.collect()\n",
    "\n",
    "print_elapsed_time(ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Word2Vec model on blog text\n",
    "Computing these models take a while.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "documents = df.tokenized_text.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Word2Vec - CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing Word2Vec CBOW model based on text of 3515 blogs\n",
      "\n",
      "Time Taken : 1.1 minutes\n",
      "\n",
      "Training: Random Forest, MeanEmbeddingVectorizer, Word2Vec CBOW, 300-Dim\n",
      "\tScore: 0.7209\n",
      "\tTime taken: 11.9 minutes\n",
      "\n",
      "Training: LinearSVC, MeanEmbeddingVectorizer, Word2Vec CBOW, 300-Dim\n",
      "\tScore: 0.7658\n",
      "\tTime taken: 6.1 minutes\n",
      "\n",
      "Training: SVM (Linear), MeanEmbeddingVectorizer, Word2Vec CBOW, 300-Dim\n",
      "\tScore: 0.7616\n",
      "\tTime taken: 3.2 minutes\n",
      "\n",
      "model                                                             score\n",
      "--------------------------------------------------------------  -------\n",
      "LinearSVC, TF-IDF, min_df 0.5%                                   0.7801\n",
      "LinearSVC, MeanEmbeddingVectorizer, GloVe medium 300-Dim         0.7744\n",
      "LinearSVC, MeanEmbeddingVectorizer, Word2Vec CBOW, 300-Dim       0.7658\n",
      "SVM (Linear), MeanEmbeddingVectorizer, Word2Vec CBOW, 300-Dim    0.7616\n",
      "SVM (Linear), MeanEmbeddingVectorizer, GloVe medium 300-Dim      0.7604\n",
      "LinearSVC, MeanEmbeddingVectorizer, GloVe small 300-Dim          0.7570\n",
      "SVM (Linear), TF-IDF, min_df 0.5%                                0.7562\n",
      "SVM (Linear), MeanEmbeddingVectorizer, GloVe small 300-Dim       0.7476\n",
      "Extra Trees, TF-IDF, min_df 0.5%                                 0.7439\n",
      "SVM (RBF), TF-IDF, min_df 0.5%                                   0.7408\n",
      "Random Forest, MeanEmbeddingVectorizer, GloVe medium 300-Dim     0.7394\n",
      "Random Forest, TF-IDF, min_df 0.5%                               0.7388\n",
      "Random Forest, MeanEmbeddingVectorizer, GloVe small 300-Dim      0.7223\n",
      "Random Forest, MeanEmbeddingVectorizer, Word2Vec CBOW, 300-Dim   0.7209\n",
      "MultinomialNB, TF-IDF, min_df 1.0%                               0.6529\n",
      "5 Nearest Neighbors, TF-IDF, min_df 0.5%                         0.6461\n",
      "MultinomialNB, TF-IDF, min_df 0.5%                               0.6452\n",
      "3 Nearest Neighbors, TF-IDF, min_df 0.5%                         0.6270\n",
      "\n",
      "Time Taken : 21.2 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ts = time.time()\n",
    "print \"Constructing Word2Vec CBOW model based on text of\", len(documents), \"blogs\"\n",
    "w2v = Word2Vec(documents, size=300, window=8, min_count=5, sg=0, workers=7)# hs=0, negative=5, cbow_mean=1\n",
    "print_elapsed_time(ts)\n",
    "\n",
    "ts = time.time()\n",
    "vec = MeanEmbeddingVectorizer({w: vec for w, vec in zip(w2v.index2word, w2v.syn0)})\n",
    "\n",
    "models_with_desc = [    \n",
    "    (\"Random Forest, MeanEmbeddingVectorizer, Word2Vec CBOW, 300-Dim\", Pipeline([(\"vec\", vec), (\"best_rf\", best_rf)])),\n",
    "    (\"LinearSVC, MeanEmbeddingVectorizer, Word2Vec CBOW, 300-Dim\", Pipeline([(\"vec\", vec), (\"best_linearsvc\", best_linearsvc)])),\n",
    "    (\"SVM (Linear), MeanEmbeddingVectorizer, Word2Vec CBOW, 300-Dim\", Pipeline([(\"vec\", vec), (\"best_svm_linear\", best_svm)]))\n",
    "]\n",
    "\n",
    "# scores = [] # Because we want to compare with the previous approaches\n",
    "scores.extend(get_cv_scores(models_with_desc))\n",
    "scores = sorted(scores, key=lambda (_, x): -x)\n",
    "print tabulate(scores, floatfmt=\".4f\", headers=(\"model\", 'score'))\n",
    "print_elapsed_time(ts)\n",
    "\n",
    "del models_with_desc\n",
    "num_unreachable_objects = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Word2Vec - Skip-gram (SG) using negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing Word2Vec model (Skip-gram using negative sampling) based on text of 3515 blogs\n",
      "\n",
      "Time Taken : 5.0 minutes\n",
      "\n",
      "Training: Random Forest, MeanEmbeddingVectorizer, Word2Vec SG + negative sampling, 300-Dim\n",
      "\tScore: 0.7331\n",
      "\tTime taken: 12.1 minutes\n",
      "\n",
      "Training: LinearSVC, MeanEmbeddingVectorizer, Word2Vec SG + negative sampling, 300-Dim\n",
      "\tScore: 0.7567\n",
      "\tTime taken: 7.2 minutes\n",
      "\n",
      "Training: SVM (Linear), MeanEmbeddingVectorizer, Word2Vec SG + negative sampling, 300-Dim\n",
      "\tScore: 0.7252\n",
      "\tTime taken: 7.4 minutes\n",
      "\n",
      "model                                                                               score\n",
      "--------------------------------------------------------------------------------  -------\n",
      "LinearSVC, TF-IDF, min_df 0.5%                                                     0.7801\n",
      "LinearSVC, MeanEmbeddingVectorizer, GloVe medium 300-Dim                           0.7744\n",
      "LinearSVC, MeanEmbeddingVectorizer, Word2Vec CBOW, 300-Dim                         0.7658\n",
      "SVM (Linear), MeanEmbeddingVectorizer, Word2Vec CBOW, 300-Dim                      0.7616\n",
      "SVM (Linear), MeanEmbeddingVectorizer, GloVe medium 300-Dim                        0.7604\n",
      "LinearSVC, MeanEmbeddingVectorizer, GloVe small 300-Dim                            0.7570\n",
      "LinearSVC, MeanEmbeddingVectorizer, Word2Vec SG + negative sampling, 300-Dim       0.7567\n",
      "SVM (Linear), TF-IDF, min_df 0.5%                                                  0.7562\n",
      "SVM (Linear), MeanEmbeddingVectorizer, GloVe small 300-Dim                         0.7476\n",
      "Extra Trees, TF-IDF, min_df 0.5%                                                   0.7439\n",
      "SVM (RBF), TF-IDF, min_df 0.5%                                                     0.7408\n",
      "Random Forest, MeanEmbeddingVectorizer, GloVe medium 300-Dim                       0.7394\n",
      "Random Forest, TF-IDF, min_df 0.5%                                                 0.7388\n",
      "Random Forest, MeanEmbeddingVectorizer, Word2Vec SG + negative sampling, 300-Dim   0.7331\n",
      "SVM (Linear), MeanEmbeddingVectorizer, Word2Vec SG + negative sampling, 300-Dim    0.7252\n",
      "Random Forest, MeanEmbeddingVectorizer, GloVe small 300-Dim                        0.7223\n",
      "Random Forest, MeanEmbeddingVectorizer, Word2Vec CBOW, 300-Dim                     0.7209\n",
      "MultinomialNB, TF-IDF, min_df 1.0%                                                 0.6529\n",
      "5 Nearest Neighbors, TF-IDF, min_df 0.5%                                           0.6461\n",
      "MultinomialNB, TF-IDF, min_df 0.5%                                                 0.6452\n",
      "3 Nearest Neighbors, TF-IDF, min_df 0.5%                                           0.6270\n",
      "\n",
      "Time Taken : 26.7 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ts = time.time()\n",
    "print \"Constructing Word2Vec model (Skip-gram using negative sampling) based on text of\", len(documents), \"blogs\"\n",
    "w2v = Word2Vec(documents, size=300, window=8, min_count=5, sg=1, hs=0, workers=7) # using negative sampling\n",
    "print_elapsed_time(ts)\n",
    "\n",
    "ts = time.time()\n",
    "vec = MeanEmbeddingVectorizer({w: vec for w, vec in zip(w2v.index2word, w2v.syn0)})\n",
    "\n",
    "models_with_desc = [    \n",
    "    (\"Random Forest, MeanEmbeddingVectorizer, Word2Vec SG + negative sampling, 300-Dim\",\n",
    "     Pipeline([(\"vec\", vec), (\"best_rf\", best_rf)])),\n",
    "    (\"LinearSVC, MeanEmbeddingVectorizer, Word2Vec SG + negative sampling, 300-Dim\",\n",
    "     Pipeline([(\"vec\", vec), (\"best_linearsvc\", best_linearsvc)])),\n",
    "    (\"SVM (Linear), MeanEmbeddingVectorizer, Word2Vec SG + negative sampling, 300-Dim\",\n",
    "     Pipeline([(\"vec\", vec), (\"best_svm_linear\", best_svm)]))\n",
    "]\n",
    "\n",
    "# scores = [] # Because we want to compare with the previous approaches\n",
    "scores.extend(get_cv_scores(models_with_desc))\n",
    "scores = sorted(scores, key=lambda (_, x): -x)\n",
    "print tabulate(scores, floatfmt=\".4f\", headers=(\"model\", 'score'))\n",
    "print_elapsed_time(ts)\n",
    "\n",
    "del models_with_desc\n",
    "num_unreachable_objects = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Word2Vec - Skip-gram (SG) using hierarchical softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing Word2Vec model (Skip-gram using hierarchical softmax) based on text of 3515 blogs\n",
      "\n",
      "Time Taken : 11.1 minutes\n",
      "\n",
      "Training: Random Forest, MeanEmbeddingVectorizer, Word2Vec SG + hierarchical softmax, 300-Dim\n",
      "\tScore: 0.7354\n",
      "\tTime taken: 12.0 minutes\n",
      "\n",
      "Training: LinearSVC, MeanEmbeddingVectorizer, Word2Vec SG + hierarchical softmax, 300-Dim\n",
      "\tScore: 0.7479\n",
      "\tTime taken: 7.0 minutes\n",
      "\n",
      "Training: SVM (Linear), MeanEmbeddingVectorizer, Word2Vec SG + hierarchical softmax, 300-Dim\n",
      "\tScore: 0.7186\n",
      "\tTime taken: 8.0 minutes\n",
      "\n",
      "model                                                                                  score\n",
      "-----------------------------------------------------------------------------------  -------\n",
      "LinearSVC, TF-IDF, min_df 0.5%                                                        0.7801\n",
      "LinearSVC, MeanEmbeddingVectorizer, GloVe medium 300-Dim                              0.7744\n",
      "LinearSVC, MeanEmbeddingVectorizer, Word2Vec CBOW, 300-Dim                            0.7658\n",
      "SVM (Linear), MeanEmbeddingVectorizer, Word2Vec CBOW, 300-Dim                         0.7616\n",
      "SVM (Linear), MeanEmbeddingVectorizer, GloVe medium 300-Dim                           0.7604\n",
      "LinearSVC, MeanEmbeddingVectorizer, GloVe small 300-Dim                               0.7570\n",
      "LinearSVC, MeanEmbeddingVectorizer, Word2Vec SG + negative sampling, 300-Dim          0.7567\n",
      "SVM (Linear), TF-IDF, min_df 0.5%                                                     0.7562\n",
      "LinearSVC, MeanEmbeddingVectorizer, Word2Vec SG + hierarchical softmax, 300-Dim       0.7479\n",
      "SVM (Linear), MeanEmbeddingVectorizer, GloVe small 300-Dim                            0.7476\n",
      "Extra Trees, TF-IDF, min_df 0.5%                                                      0.7439\n",
      "SVM (RBF), TF-IDF, min_df 0.5%                                                        0.7408\n",
      "Random Forest, MeanEmbeddingVectorizer, GloVe medium 300-Dim                          0.7394\n",
      "Random Forest, TF-IDF, min_df 0.5%                                                    0.7388\n",
      "Random Forest, MeanEmbeddingVectorizer, Word2Vec SG + hierarchical softmax, 300-Dim   0.7354\n",
      "Random Forest, MeanEmbeddingVectorizer, Word2Vec SG + negative sampling, 300-Dim      0.7331\n",
      "SVM (Linear), MeanEmbeddingVectorizer, Word2Vec SG + negative sampling, 300-Dim       0.7252\n",
      "Random Forest, MeanEmbeddingVectorizer, GloVe small 300-Dim                           0.7223\n",
      "Random Forest, MeanEmbeddingVectorizer, Word2Vec CBOW, 300-Dim                        0.7209\n",
      "SVM (Linear), MeanEmbeddingVectorizer, Word2Vec SG + hierarchical softmax, 300-Dim    0.7186\n",
      "MultinomialNB, TF-IDF, min_df 1.0%                                                    0.6529\n",
      "5 Nearest Neighbors, TF-IDF, min_df 0.5%                                              0.6461\n",
      "MultinomialNB, TF-IDF, min_df 0.5%                                                    0.6452\n",
      "3 Nearest Neighbors, TF-IDF, min_df 0.5%                                              0.6270\n",
      "\n",
      "Time Taken : 27.0 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ts = time.time()\n",
    "print \"Constructing Word2Vec model (Skip-gram using hierarchical softmax) based on text of\", len(documents), \"blogs\"\n",
    "w2v = Word2Vec(documents, size=300, window=8, min_count=5, sg=1, hs=1, workers=7) # using hierarchical softmax\n",
    "print_elapsed_time(ts)\n",
    "\n",
    "ts = time.time()\n",
    "vec = MeanEmbeddingVectorizer({w: vec for w, vec in zip(w2v.index2word, w2v.syn0)})\n",
    "\n",
    "models_with_desc = [    \n",
    "    (\"Random Forest, MeanEmbeddingVectorizer, Word2Vec SG + hierarchical softmax, 300-Dim\",\n",
    "     Pipeline([(\"vec\", vec), (\"best_rf\", best_rf)])),\n",
    "    (\"LinearSVC, MeanEmbeddingVectorizer, Word2Vec SG + hierarchical softmax, 300-Dim\",\n",
    "     Pipeline([(\"vec\", vec), (\"best_linearsvc\", best_linearsvc)])),\n",
    "    (\"SVM (Linear), MeanEmbeddingVectorizer, Word2Vec SG + hierarchical softmax, 300-Dim\",\n",
    "     Pipeline([(\"vec\", vec), (\"best_svm_linear\", best_svm)]))\n",
    "]\n",
    "\n",
    "# scores = [] # Because we want to compare with the previous approaches\n",
    "scores.extend(get_cv_scores(models_with_desc))\n",
    "scores = sorted(scores, key=lambda (_, x): -x)\n",
    "print tabulate(scores, floatfmt=\".4f\", headers=(\"model\", 'score'))\n",
    "print_elapsed_time(ts)\n",
    "\n",
    "del models_with_desc\n",
    "num_unreachable_objects = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3515"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Word2Vec` model was built on a dataset containing blog posts written by 3515 authors - this probably explains the low scores.\n",
    "### 3.3 Use pretrained Word2Vec model on the Google News (100B) corpus\n",
    "There is a GitHub repository (https://github.com/3Top/word2vec-api#where-to-get-a-pretrained-models) listing several pretrained models. I am using the 300-dimension model trained using negative sampling (size 1.5GB, 3M words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the pretrained Word2Vec model, GoogleNews-vectors-negative300.bin.gz, into gensim\n",
      "\n",
      "Time Taken : 7.3 minutes\n",
      "\n",
      "Training: Random Forest, MeanEmbeddingVectorizer, Word2Vec SG + negative sampling, 300-Dim (pretrained-100B)\n",
      "\tScore: 0.7360\n",
      "\tTime taken: 15.6 minutes\n",
      "\n",
      "Training: LinearSVC, MeanEmbeddingVectorizer, Word2Vec SG + negative sampling, 300-Dim (pretrained-100B)\n",
      "\tScore: 0.7565\n",
      "\tTime taken: 11.5 minutes\n",
      "\n",
      "Training: SVM (Linear), MeanEmbeddingVectorizer, Word2Vec SG + negative sampling, 300-Dim (pretrained-100B)\n",
      "\tScore: 0.7234\n",
      "\tTime taken: 11.7 minutes\n",
      "\n",
      "model                                                                                                 score\n",
      "--------------------------------------------------------------------------------------------------  -------\n",
      "LinearSVC, TF-IDF, min_df 0.5%                                                                       0.7801\n",
      "LinearSVC, MeanEmbeddingVectorizer, GloVe medium 300-Dim                                             0.7744\n",
      "LinearSVC, MeanEmbeddingVectorizer, Word2Vec CBOW, 300-Dim                                           0.7658\n",
      "SVM (Linear), MeanEmbeddingVectorizer, Word2Vec CBOW, 300-Dim                                        0.7616\n",
      "SVM (Linear), MeanEmbeddingVectorizer, GloVe medium 300-Dim                                          0.7604\n",
      "LinearSVC, MeanEmbeddingVectorizer, GloVe small 300-Dim                                              0.7570\n",
      "LinearSVC, MeanEmbeddingVectorizer, Word2Vec SG + negative sampling, 300-Dim                         0.7567\n",
      "LinearSVC, MeanEmbeddingVectorizer, Word2Vec SG + negative sampling, 300-Dim (pretrained-100B)       0.7565\n",
      "SVM (Linear), TF-IDF, min_df 0.5%                                                                    0.7562\n",
      "LinearSVC, MeanEmbeddingVectorizer, Word2Vec SG + hierarchical softmax, 300-Dim                      0.7479\n",
      "SVM (Linear), MeanEmbeddingVectorizer, GloVe small 300-Dim                                           0.7476\n",
      "Extra Trees, TF-IDF, min_df 0.5%                                                                     0.7439\n",
      "SVM (RBF), TF-IDF, min_df 0.5%                                                                       0.7408\n",
      "Random Forest, MeanEmbeddingVectorizer, GloVe medium 300-Dim                                         0.7394\n",
      "Random Forest, TF-IDF, min_df 0.5%                                                                   0.7388\n",
      "Random Forest, MeanEmbeddingVectorizer, Word2Vec SG + negative sampling, 300-Dim (pretrained-100B)   0.7360\n",
      "Random Forest, MeanEmbeddingVectorizer, Word2Vec SG + hierarchical softmax, 300-Dim                  0.7354\n",
      "Random Forest, MeanEmbeddingVectorizer, Word2Vec SG + negative sampling, 300-Dim                     0.7331\n",
      "SVM (Linear), MeanEmbeddingVectorizer, Word2Vec SG + negative sampling, 300-Dim                      0.7252\n",
      "SVM (Linear), MeanEmbeddingVectorizer, Word2Vec SG + negative sampling, 300-Dim (pretrained-100B)    0.7234\n",
      "Random Forest, MeanEmbeddingVectorizer, GloVe small 300-Dim                                          0.7223\n",
      "Random Forest, MeanEmbeddingVectorizer, Word2Vec CBOW, 300-Dim                                       0.7209\n",
      "SVM (Linear), MeanEmbeddingVectorizer, Word2Vec SG + hierarchical softmax, 300-Dim                   0.7186\n",
      "MultinomialNB, TF-IDF, min_df 1.0%                                                                   0.6529\n",
      "5 Nearest Neighbors, TF-IDF, min_df 0.5%                                                             0.6461\n",
      "MultinomialNB, TF-IDF, min_df 0.5%                                                                   0.6452\n",
      "3 Nearest Neighbors, TF-IDF, min_df 0.5%                                                             0.6270\n",
      "\n",
      "Time Taken : 38.9 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the pretrained Word2Vec model into gensim\n",
    "ts = time.time()\n",
    "print \"Loading the pretrained Word2Vec model, GoogleNews-vectors-negative300.bin.gz, into gensim\"\n",
    "w2v = Word2Vec.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "print_elapsed_time(ts)\n",
    "\n",
    "ts = time.time()\n",
    "vec = MeanEmbeddingVectorizer({w: vec for w, vec in zip(w2v.index2word, w2v.syn0)})\n",
    "\n",
    "models_with_desc = [    \n",
    "    (\"Random Forest, MeanEmbeddingVectorizer, Word2Vec SG + negative sampling, 300-Dim (pretrained-100B)\",\n",
    "     Pipeline([(\"vec\", vec), (\"best_rf\", best_rf)])),\n",
    "    (\"LinearSVC, MeanEmbeddingVectorizer, Word2Vec SG + negative sampling, 300-Dim (pretrained-100B)\",\n",
    "     Pipeline([(\"vec\", vec), (\"best_linearsvc\", best_linearsvc)])),\n",
    "    (\"SVM (Linear), MeanEmbeddingVectorizer, Word2Vec SG + negative sampling, 300-Dim (pretrained-100B)\",\n",
    "     Pipeline([(\"vec\", vec), (\"best_svm_linear\", best_svm)]))\n",
    "]\n",
    "\n",
    "# scores = [] # Because we want to compare with the previous approaches\n",
    "scores.extend(get_cv_scores(models_with_desc))\n",
    "scores = sorted(scores, key=lambda (_, x): -x)\n",
    "print tabulate(scores, floatfmt=\".4f\", headers=(\"model\", 'score'))\n",
    "print_elapsed_time(ts)\n",
    "\n",
    "del models_with_desc\n",
    "del vec\n",
    "del w2v\n",
    "num_unreachable_objects = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `GloVe` & `Word2Vec` Slightly Lower Performace Than BoW - Expected Higher\n",
    "- `63.55%` (Best in part 1, no preprocessing + smaller dataset).\n",
    "- **`78.01%`** (BoW, basic preprocessing on text)\n",
    "- `77.44%` (using word vectors - GloVe *medium*, basic preprocessing on text)\n",
    "- `76.58%` Word2Vec CBOW model on blog text\n",
    "- `75.65%` using pretrained Word2Vec model on the Google News (100B) corpus\n",
    "\n",
    "## Current TODO\n",
    "- Need to investigate **why no significant improvements using word vectors**\n",
    "- Further preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
