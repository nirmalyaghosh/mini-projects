{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "# Predicting Gender Based On Blog Text - Part 3 - Doc2Vec\n",
    "\n",
    "A comparison of a few solutions for identifying the gender of blog authors based on his/her writing style. It is based on a dataset containing 681288 blog posts downloaded from <a href=\\\"http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm\\\" target=\"_blank\">here</a>. I am using a smaller dataset containing 145044 blog posts written by 3074 authors (1716 female, 1358 male) in the 24-25 age group.\n",
    "\n",
    "This is the 3rd in a series of notebooks.\n",
    "\n",
    "In this notebook, I compare the 2 types of [Doc2Vec](#2.-Doc2Vec) models\n",
    "- [*distributed memory*](#2.1-Doc2Vec---distributed-memory) (PV-DM) and\n",
    "- [*distributed bag of words*](#2.3-Doc2Vec---distributed-bag-of-words) (PV-DBOW)\n",
    "\n",
    "For the impatient, [here](#3.-Final-Comparison) are the results.\n",
    "\n",
    "*The hyperlinks should help navigate through this notebook*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gensim : 0.12.4\n",
      "numpy : 1.10.4\n",
      "pandas : 0.18.1\n",
      "sklearn : 0.17.1\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import ast\n",
    "import gc\n",
    "import gensim\n",
    "import logging\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import time\n",
    "import yaml\n",
    "\n",
    "from genderpredictutils import dataprep, textpreprocess, trainingutils\n",
    "from gensim import parsing, utils\n",
    "from gensim.models.doc2vec import Doc2Vec, LabeledSentence\n",
    "from imp import reload\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.linear_model import RidgeClassifier, SGDClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from tabulate import tabulate\n",
    "\n",
    "_random_state = 371250\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "# Print versions\n",
    "print(\"gensim : {}\".format(gensim.__version__))\n",
    "print(\"numpy : {}\".format(np.__version__))\n",
    "print(\"pandas : {}\".format(pd.__version__))\n",
    "print(\"sklearn : {}\".format(sklearn.__version__))\n",
    "\n",
    "reload(logging)\n",
    "logging.basicConfig(format=\"%(asctime)s: %(message)s\", level=logging.INFO, datefmt=\"%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"which_gender.yml\", \"r\") as f:\n",
    "    cfg = yaml.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Reading the dataset\n",
    "\n",
    "Each author's posts appear as a separate file. The name indicates blogger id#, self-provided gender, age, industry and astrological sign. (All are labeled for gender and age but for many, industry and/or sign is marked as unknown.)\n",
    "\n",
    "The work for reading the XML files from the .zip file has been done by the dataprep module. So, just reusing the pre-created dataset and filtering out authors not in the 24-25 age group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = cfg[\"common\"][\"data_dir\"]\n",
    "models_dir = cfg[\"common\"][\"models_dir\"]\n",
    "d2v_models_dir = os.path.join(models_dir, \"d2v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Read the `gz` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145044, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path_1 = os.path.join(data_dir, \"blog_posts_metadata.txt.gz\")\n",
    "file_path_2 = os.path.join(data_dir, \"blog_posts.txt.gz\")\n",
    "\n",
    "if os.path.exists(file_path_1) == False or os.path.exists(file_path_2) == False:\n",
    "    print(\"One (or both) of {} or {} does not exist, so creating them\".format(file_path_1, file_path_2))\n",
    "    file_paths = dataprep.prepare_data(data_dir, num_processes=6) # This takes a while\n",
    "    file_path_1, file_path_2 = file_paths[0], file_paths[1]\n",
    "\n",
    "df0 = pd.read_csv(file_path_1, usecols=[\"blogger_id\", \"gender\", \"age\"], sep=\"\\t\", index_col=False)\n",
    "target_age_grp = df0[df0[\"age\"].isin([24,25])][\"blogger_id\"].values.tolist()\n",
    "\n",
    "df_iter = pd.read_csv(file_path_2, sep=\"\\t\", index_col=False, iterator=True, chunksize=1000)\n",
    "df = pd.concat([chunk[chunk[\"blogger_id\"].isin(target_age_grp)] for chunk in df_iter])\n",
    "df = pd.merge(df0, df)\n",
    "df = df.dropna(subset=[\"blog_post\"])\n",
    "df = df.sort_values(by=\"blogger_id\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145044, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get rid of columns we do not need\n",
    "df = df[[\"blogger_id\", \"gender\", \"date\", \"blog_post\"]]\n",
    "num_unreachable_objects = gc.collect()\n",
    "df = df.dropna(subset=[\"blog_post\"])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blogger_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>date</th>\n",
       "      <th>blog_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>119689</th>\n",
       "      <td>5114</td>\n",
       "      <td>male</td>\n",
       "      <td>2002-11-06</td>\n",
       "      <td>Sign #249  urlLink CNN  needs some sense slapp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119722</th>\n",
       "      <td>5114</td>\n",
       "      <td>male</td>\n",
       "      <td>2004-04-19</td>\n",
       "      <td>The new issue of Mindjack is  urlLink now onli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119723</th>\n",
       "      <td>5114</td>\n",
       "      <td>male</td>\n",
       "      <td>2004-04-12</td>\n",
       "      <td>There's a  urlLink new issue  of Mindjack now ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        blogger_id gender        date  \\\n",
       "119689        5114   male  2002-11-06   \n",
       "119722        5114   male  2004-04-19   \n",
       "119723        5114   male  2004-04-12   \n",
       "\n",
       "                                                blog_post  \n",
       "119689  Sign #249  urlLink CNN  needs some sense slapp...  \n",
       "119722  The new issue of Mindjack is  urlLink now onli...  \n",
       "119723  There's a  urlLink new issue  of Mindjack now ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Encode the gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:21:27: ['female', 'male']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blogger_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>date</th>\n",
       "      <th>blog_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>119689</th>\n",
       "      <td>5114</td>\n",
       "      <td>1</td>\n",
       "      <td>2002-11-06</td>\n",
       "      <td>Sign #249  urlLink CNN  needs some sense slapp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119722</th>\n",
       "      <td>5114</td>\n",
       "      <td>1</td>\n",
       "      <td>2004-04-19</td>\n",
       "      <td>The new issue of Mindjack is  urlLink now onli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119723</th>\n",
       "      <td>5114</td>\n",
       "      <td>1</td>\n",
       "      <td>2004-04-12</td>\n",
       "      <td>There's a  urlLink new issue  of Mindjack now ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        blogger_id  gender        date  \\\n",
       "119689        5114       1  2002-11-06   \n",
       "119722        5114       1  2004-04-19   \n",
       "119723        5114       1  2004-04-12   \n",
       "\n",
       "                                                blog_post  \n",
       "119689  Sign #249  urlLink CNN  needs some sense slapp...  \n",
       "119722  The new issue of Mindjack is  urlLink now onli...  \n",
       "119723  There's a  urlLink new issue  of Mindjack now ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_enc = LabelEncoder()\n",
    "gender_enc.fit(df.gender.values.tolist())\n",
    "logging.info(list(gender_enc.classes_))\n",
    "df[\"gender\"] = gender_enc.transform(df.gender.values.tolist())\n",
    "df.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Preprocess the text\n",
    "This takes a while. I noticed **each instance** of Spacy English parser takes up **~3GB of RAM** (also verified it, https://github.com/spacy-io/spaCy/issues/100), so set the number of processes prudently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:21:27: Reading tokenized_text.txt\n",
      "10:22:09: (145044, 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.7 s, sys: 1.16 s, total: 41.8 s\n",
      "Wall time: 42.6 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blogger_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>date</th>\n",
       "      <th>blog_post</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5114</td>\n",
       "      <td>1</td>\n",
       "      <td>2002-11-06</td>\n",
       "      <td>Sign #249  urlLink CNN  needs some sense slapp...</td>\n",
       "      <td>[sign, 249, cnn, need, sense, slap, day, elect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5114</td>\n",
       "      <td>1</td>\n",
       "      <td>2004-04-19</td>\n",
       "      <td>The new issue of Mindjack is  urlLink now onli...</td>\n",
       "      <td>[new, issue, mindjack, online, link, blogging,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5114</td>\n",
       "      <td>1</td>\n",
       "      <td>2004-04-12</td>\n",
       "      <td>There's a  urlLink new issue  of Mindjack now ...</td>\n",
       "      <td>[new, issue, mindjack, online, article, mindja...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   blogger_id  gender        date  \\\n",
       "0        5114       1  2002-11-06   \n",
       "1        5114       1  2004-04-19   \n",
       "2        5114       1  2004-04-12   \n",
       "\n",
       "                                           blog_post  \\\n",
       "0  Sign #249  urlLink CNN  needs some sense slapp...   \n",
       "1  The new issue of Mindjack is  urlLink now onli...   \n",
       "2  There's a  urlLink new issue  of Mindjack now ...   \n",
       "\n",
       "                                      tokenized_text  \n",
       "0  [sign, 249, cnn, need, sense, slap, day, elect...  \n",
       "1  [new, issue, mindjack, online, link, blogging,...  \n",
       "2  [new, issue, mindjack, online, article, mindja...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = \"tokenized_text.txt\"\n",
    "if os.path.exists(tokenized_dataset) == False:\n",
    "    %time df = textpreprocess.tokenize_text(df, col_name=\"blog_post\", num_processes=cfg[\"tokenize_text\"][\"num_processes\"])\n",
    "    df.to_csv(tokenized_dataset, sep=\"\\t\", index=False)\n",
    "else:\n",
    "    logging.info(\"Reading {}\".format(tokenized_dataset))\n",
    "    %time df = pd.read_csv(tokenized_dataset, sep=\"\\t\", converters={\"tokenized_text\":ast.literal_eval})\n",
    "\n",
    "if len(df.columns.values.tolist()) >= 6:\n",
    "    df.drop([\"Unnamed: 0\"], axis=1, inplace=True)\n",
    "\n",
    "logging.info(df.shape)\n",
    "df.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.01 s, sys: 0 ns, total: 7.01 s\n",
      "Wall time: 7.02 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blogger_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>date</th>\n",
       "      <th>blog_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5114</td>\n",
       "      <td>1</td>\n",
       "      <td>2002-11-06</td>\n",
       "      <td>sign 249 cnn need sense slap day election repu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5114</td>\n",
       "      <td>1</td>\n",
       "      <td>2004-04-19</td>\n",
       "      <td>new issue mindjack online link blogging equali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5114</td>\n",
       "      <td>1</td>\n",
       "      <td>2004-04-12</td>\n",
       "      <td>new issue mindjack online article mindjack 's ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   blogger_id  gender        date  \\\n",
       "0        5114       1  2002-11-06   \n",
       "1        5114       1  2004-04-19   \n",
       "2        5114       1  2004-04-12   \n",
       "\n",
       "                                           blog_post  \n",
       "0  sign 249 cnn need sense slap day election repu...  \n",
       "1  new issue mindjack online link blogging equali...  \n",
       "2  new issue mindjack online article mindjack 's ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the tokens into a single string - as required downstream\n",
    "def func_concat_tokens(x):\n",
    "    terms = x[\"tokenized_text\"]\n",
    "    terms = [str(t) for t in terms]\n",
    "    return \" \".join(terms)\n",
    "%time df[\"tokenized_text_rejoined\"] = df.apply(func_concat_tokens , axis=1)\n",
    "\n",
    "# Next, replace the text within \"blog_post\" with text in \"tokenized_text_rejoined\"\n",
    "df[\"blog_post\"] = df[\"tokenized_text_rejoined\"]\n",
    "df.drop([\"tokenized_text\", \"tokenized_text_rejoined\"], axis=1, inplace=True)\n",
    "del textpreprocess._spacy_parser_\n",
    "num_unreachable_objects = gc.collect()\n",
    "df.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Feature Extraction\n",
    "How many features do we have?\n",
    "\n",
    "- I ran TF-IDF vectorizer with minimum document frequency 0.1%, 0.5% and 1.0% to get 6851, 2069 and 1104 features respectively.\n",
    "- I then repeated with maximum document frequency in the range 10 - 50%, to get ~240K features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:22:36: max_df = 0.5, X.shape : (145044, 239936), len(y) : 145044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.8 s, sys: 271 ms, total: 19 s\n",
      "Wall time: 19.1 s\n"
     ]
    }
   ],
   "source": [
    "# Choosing max_df=0.5, which gives ~240K features - feature selection will be done later\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words=\"english\")\n",
    "%time X = vectorizer.fit_transform(df[\"blog_post\"]) # sparse matrix in CSR format\n",
    "y = np.array(df.gender.values.tolist())\n",
    "logging.info(\"max_df = 0.5, X.shape : {}, len(y) : {}\".format(X.shape, len(y)))\n",
    "num_unreachable_objects = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Back to [Top](#top)\n",
    "# 2. Doc2Vec\n",
    "Apply variants of the 2 types of Doc2Vec models,\n",
    "- *distributed memory* (PV-DM)\n",
    " - [first](#2.1-Doc2Vec---distributed-memory), I try various windows sizes - for different feature vector lengths (100, 200, 300)\n",
    " - [next](#2.2-Doc2Vec---distributed-memory---2nd-try), I manipulate the `sample` and `negative` parameters - also for different feature vector lengths (100, 200, 300)\n",
    "- [*distributed bag of words*](#2.3-Doc2Vec---distributed-bag-of-words) (PV-DBOW)\n",
    "\n",
    "First, get the text of the blog posts in the format required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:23:59: 145044 documents\n"
     ]
    }
   ],
   "source": [
    "# Get the text of the blog posts in the format required\n",
    "documents = df.blog_post.values.tolist() # this is a list of strings\n",
    "documents = [str(x).split() for x in documents] # this is a list of lists of tokens\n",
    "logging.info(\"{} documents\".format(len(documents)))\n",
    "\n",
    "# Prepare the LabeledSentences required by Doc2Vec\n",
    "d2v_sentences = []\n",
    "for i, item in enumerate(documents):\n",
    "    sentence = LabeledSentence(item, [u\"SENT_{}\".format(i)])\n",
    "    d2v_sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define common helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d2v_dim = 100\n",
    "\n",
    "def train_doc2vec_models(d2v_models):\n",
    "    trained_d2v_models = []\n",
    "    for model_id, doc2vec_dim, model in d2v_models:\n",
    "        model_file_path = os.path.join(d2v_models_dir, \"{}.doc2vec\".format(model_id))\n",
    "        model, model_id = trainingutils.train_doc2vec_model(model, model_id, d2v_sentences, model_file_path)\n",
    "        trained_d2v_models.append((model_id, doc2vec_dim, model_file_path))\n",
    "    \n",
    "    return trained_d2v_models\n",
    "\n",
    "def train_classifiers_on_vectors_from_d2v_model(d2v_models, top_n=10):\n",
    "    scores = []\n",
    "    for model_id, doc2vec_dim, model_file_path in d2v_models:\n",
    "        # First, load the model from file\n",
    "        model = Doc2Vec().load(model_file_path)\n",
    "        \n",
    "        # Next, split the vectors into random train and test subsets -- vectors are specific to current model\n",
    "        train_arrays, train_labels, test_arrays, test_labels = \\\n",
    "            trainingutils.get_doc2vec_train_test_data(model, doc2vec_dim, y, _random_state)\n",
    "        \n",
    "        # Next, rename the classifier id strings based on the model_id\n",
    "        _clfs = [(x[0] + \", \" + model_id, x[1]) for x in clfs]\n",
    "        \n",
    "        # Train the classifiers\n",
    "        logging.info(\"Training classifiers using vectors from Doc2Vec model, {}\".format(model_id))\n",
    "        model_specific_scores = []\n",
    "        for clf_id, clf in _clfs:\n",
    "            cv_score = trainingutils.get_cv_score(clf_id, clf, train_arrays, train_labels, n_jobs=num_cores)\n",
    "            logging.info(\"{}, {:.4f}\".format(clf_id, cv_score))\n",
    "            model_specific_scores.append(cv_score)\n",
    "            scores.append((clf_id, cv_score))\n",
    "    \n",
    "        logging.info(\"Trained classifiers using vectors from Doc2Vec model, {}\".format(model_id))\n",
    "        logging.info(\"Average F1-score: {:.4f}\".format(np.mean(model_specific_scores)))\n",
    "\n",
    "        # Convert the scores into a DF\n",
    "        scores_df = pd.DataFrame(scores, columns=[\"Model\", \"F1-score\"])\n",
    "        \n",
    "        # Read the older scores - to compare\n",
    "        scores_file_path = \"scores.txt\"\n",
    "        if os.path.exists(scores_file_path):\n",
    "            tmp_df = pd.read_csv(scores_file_path, sep=\"\\t\")\n",
    "            # Concat with scores of previous runs\n",
    "            scores_df = pd.concat([scores_df, tmp_df])\n",
    "        \n",
    "        scores_df = scores_df.drop_duplicates(subset=[\"Model\"], keep=\"last\")\n",
    "        scores_df = scores_df.sort_values(by=[\"F1-score\"], ascending=[0])\n",
    "        scores_df.index = range(1, len(scores_df) + 1)\n",
    "        scores_df.to_csv(scores_file_path, sep=\"\\t\", index=False, float_format=\"%.4f\")\n",
    "        \n",
    "        # Free up some RAM\n",
    "        del model\n",
    "        num_unreachable_objects = gc.collect()\n",
    "\n",
    "    scores = sorted(scores, key=lambda x: -x[1])\n",
    "    # Tabulating only the top N scores, the rest are too low to be considered\n",
    "    print(tabulate(scores[:top_n], floatfmt=\".4f\", headers=(\"Model\", \"F1-score\")))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Classifiers being used to compare the various Doc2Vec models\n",
    "clfs = [\n",
    "    (\"ExtraTrees_600, Doc2Vec\", ExtraTreesClassifier(n_estimators=600, random_state=_random_state)),\n",
    "    (\"LinearSVC_07, Doc2Vec\", LinearSVC(C=0.7, random_state=_random_state)),\n",
    "    (\"LogisticRegression, Doc2Vec\", LogisticRegression()),\n",
    "    (\"LogisticRegressionCV_sag, Doc2Vec\", LogisticRegressionCV(cv=5, Cs=list(np.power(10.0, np.arange(-10, 10))),\n",
    "                                                               random_state=_random_state, solver=\"sag\")),\n",
    "    (\"PassiveAggressive_01, Doc2Vec\", PassiveAggressiveClassifier(C=0.1, n_iter=50, random_state=_random_state)),\n",
    "    (\"RandomForest_600, Doc2Vec\", RandomForestClassifier(n_estimators=600, random_state=_random_state)),\n",
    "    (\"RidgeClassifier-auto-1e-3, Doc2Vec\", RidgeClassifier(tol=1e-3, solver=\"auto\", random_state=_random_state)),\n",
    "    (\"SGD_elasticnet_penalty, Doc2Vec\", \n",
    "     SGDClassifier(alpha=.0001, n_iter=150, penalty=\"elasticnet\", random_state=_random_state)),\n",
    "    (\"SGD_l1_penalty, Doc2Vec\", SGDClassifier(alpha=.0001, n_iter=150, penalty=\"l1\", random_state=_random_state)),\n",
    "    (\"SGD_l2_penalty, Doc2Vec\", SGDClassifier(alpha=.0001, n_iter=150, penalty=\"l2\", random_state=_random_state)),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Back to [Top](#top), [Doc2Vec](#2.-Doc2Vec)\n",
    "### 2.1 Doc2Vec - *distributed memory*\n",
    "\n",
    "Train the **Doc2Vec-DM** models (*varying window sizes and length of feature vectors*) and then train the classifiers using the vectors from these models.\n",
    "\n",
    "Training the models take time (30+ minutes), so in most cases I load (from file) previosuly trained models. I set `min_count`=1 because each post is being treated as a sentence with a label (example `SENT123`) which appears just once.\n",
    "\n",
    "Best [Average F1-scores](#2.1.1-Doc2Vec-DM-Average-F1-scores) and [Maximum F1-scores](#2.1.2-Doc2Vec-DM-Maximum-F1-scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05:46:59: Doc2Vec model 'model1_dms_d100_hs_w20', Doc2Vec(dm/s,d100,hs,w20,t12)\n",
      "05:46:59: Loading from models/d2v/model1_dms_d100_hs_w20.doc2vec ...\n",
      "05:47:09: Doc2Vec model 'model1_dms_d200_hs_w20', Doc2Vec(dm/s,d200,hs,w20,t12)\n",
      "05:47:09: Loading from models/d2v/model1_dms_d200_hs_w20.doc2vec ...\n",
      "05:47:29: Training classifiers using vectors from Doc2Vec model, model1_dms_d100_hs_w20\n",
      "05:51:46: ExtraTrees_600, Doc2Vec, model1_dms_d100_hs_w20, 0.7044\n",
      "05:52:28: LinearSVC_07, Doc2Vec, model1_dms_d100_hs_w20, 0.6918\n",
      "05:52:35: LogisticRegression, Doc2Vec, model1_dms_d100_hs_w20, 0.6911\n",
      "05:53:53: LogisticRegressionCV_sag, Doc2Vec, model1_dms_d100_hs_w20, 0.6913\n",
      "05:54:02: PassiveAggressive_01, Doc2Vec, model1_dms_d100_hs_w20, 0.6808\n",
      "06:05:51: RandomForest_600, Doc2Vec, model1_dms_d100_hs_w20, 0.6946\n",
      "06:05:57: RidgeClassifier-auto-1e-3, Doc2Vec, model1_dms_d100_hs_w20, 0.6921\n",
      "06:06:18: SGD_elasticnet_penalty, Doc2Vec, model1_dms_d100_hs_w20, 0.6949\n",
      "06:06:36: SGD_l1_penalty, Doc2Vec, model1_dms_d100_hs_w20, 0.6948\n",
      "06:06:50: SGD_l2_penalty, Doc2Vec, model1_dms_d100_hs_w20, 0.6949\n",
      "06:06:50: Trained classifiers using vectors from Doc2Vec model, model1_dms_d100_hs_w20\n",
      "06:06:50: Average F1-score: 0.6931\n",
      "06:07:03: Training classifiers using vectors from Doc2Vec model, model1_dms_d200_hs_w20\n",
      "06:12:39: ExtraTrees_600, Doc2Vec, model1_dms_d200_hs_w20, 0.6994\n",
      "06:13:31: LinearSVC_07, Doc2Vec, model1_dms_d200_hs_w20, 0.6944\n",
      "06:13:40: LogisticRegression, Doc2Vec, model1_dms_d200_hs_w20, 0.6935\n",
      "06:15:49: LogisticRegressionCV_sag, Doc2Vec, model1_dms_d200_hs_w20, 0.6944\n",
      "06:16:01: PassiveAggressive_01, Doc2Vec, model1_dms_d200_hs_w20, 0.6832\n",
      "06:32:41: RandomForest_600, Doc2Vec, model1_dms_d200_hs_w20, 0.6886\n",
      "06:32:48: RidgeClassifier-auto-1e-3, Doc2Vec, model1_dms_d200_hs_w20, 0.6947\n",
      "06:33:21: SGD_elasticnet_penalty, Doc2Vec, model1_dms_d200_hs_w20, 0.6972\n",
      "06:33:50: SGD_l1_penalty, Doc2Vec, model1_dms_d200_hs_w20, 0.6972\n",
      "06:34:08: SGD_l2_penalty, Doc2Vec, model1_dms_d200_hs_w20, 0.6972\n",
      "06:34:08: Trained classifiers using vectors from Doc2Vec model, model1_dms_d200_hs_w20\n",
      "06:34:08: Average F1-score: 0.6940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model                                                      F1-score\n",
      "-------------------------------------------------------  ----------\n",
      "ExtraTrees_600, Doc2Vec, model1_dms_d100_hs_w20              0.7044\n",
      "ExtraTrees_600, Doc2Vec, model1_dms_d200_hs_w20              0.6994\n",
      "SGD_elasticnet_penalty, Doc2Vec, model1_dms_d200_hs_w20      0.6972\n",
      "SGD_l2_penalty, Doc2Vec, model1_dms_d200_hs_w20              0.6972\n",
      "SGD_l1_penalty, Doc2Vec, model1_dms_d200_hs_w20              0.6972\n"
     ]
    }
   ],
   "source": [
    "# Train the `distributed memory` (PV-DM) models\n",
    "# Model 1 : distributed memory (dm=1), vary the dimensionality of feature vectors along with the window size\n",
    "\n",
    "# NOTE: All models listed below (including the commented ones) have been run previously.\n",
    "#       Using the uncommented models for demonstration because they have the highest Average F1-scores.\n",
    "\n",
    "models = [\n",
    "    # 100 dimension feature vectors\n",
    "    #(\"model1_dms_d100_hs_w5\", d2v_dim*1, Doc2Vec(alpha=0.025, min_alpha=0.025, min_count=1, size=d2v_dim*1, window=5,\n",
    "    #                                             workers=num_cores)),\n",
    "    #(\"model1_dms_d100_hs_w8\", d2v_dim*1, Doc2Vec(alpha=0.025, min_alpha=0.025, min_count=1, size=d2v_dim*1, window=8,\n",
    "    #                                              workers=num_cores)),\n",
    "    #(\"model1_dms_d100_hs_w10\", d2v_dim*1, Doc2Vec(alpha=0.025, min_alpha=0.025, min_count=1, size=d2v_dim*1, window=10,\n",
    "    #                                              workers=num_cores)),\n",
    "    (\"model1_dms_d100_hs_w20\", d2v_dim*1, Doc2Vec(alpha=0.025, min_alpha=0.025, min_count=1, size=d2v_dim*1, window=20,\n",
    "                                                  workers=num_cores)),\n",
    "    \n",
    "    # 200 dimension feature vectors\n",
    "    #(\"model1_dms_d200_hs_w5\", d2v_dim*2, Doc2Vec(alpha=0.025, min_alpha=0.025, min_count=1, size=d2v_dim*2, window=5,\n",
    "    #                                             workers=num_cores)),\n",
    "    #(\"model1_dms_d200_hs_w8\", d2v_dim*2, Doc2Vec(alpha=0.025, min_alpha=0.025, min_count=1, size=d2v_dim*2, window=8,\n",
    "    #                                             workers=num_cores)),\n",
    "    #(\"model1_dms_d200_hs_w10\", d2v_dim*2, Doc2Vec(alpha=0.025, min_alpha=0.025, min_count=1, size=d2v_dim*2, window=10,\n",
    "    #                                              workers=num_cores)),\n",
    "    (\"model1_dms_d200_hs_w20\", d2v_dim*2, Doc2Vec(alpha=0.025, min_alpha=0.025, min_count=1, size=d2v_dim*2, window=20,\n",
    "                                                  workers=num_cores)),\n",
    "    \n",
    "    # 300 dimension feature vectors\n",
    "    #(\"model1_dms_d300_hs_w5\", d2v_dim*3, Doc2Vec(alpha=0.025, min_alpha=0.025, min_count=1, size=d2v_dim*3, window=5,\n",
    "    #                                             workers=num_cores)),\n",
    "    #(\"model1_dms_d300_hs_w8\", d2v_dim*3, Doc2Vec(alpha=0.025, min_alpha=0.025, min_count=1, size=d2v_dim*3, window=8,\n",
    "    #                                             workers=num_cores)),\n",
    "    #(\"model1_dms_d300_hs_w10\", d2v_dim*3, Doc2Vec(alpha=0.025, min_alpha=0.025, min_count=1, size=d2v_dim*3, window=10,\n",
    "    #                                              workers=num_cores)),\n",
    "    #(\"model1_dms_d300_hs_w20\", d2v_dim*3, Doc2Vec(alpha=0.025, min_alpha=0.025, min_count=1, size=d2v_dim*3, window=20,\n",
    "    #                                              workers=num_cores)),\n",
    "]\n",
    "\n",
    "trained_dm_models = train_doc2vec_models(models)\n",
    "\n",
    "# Train the classifiers using the vectors from the Doc2Vec-DM models (model 1 variants),\n",
    "# show only top 5 highest F1-scores\n",
    "scores_dm_1 = train_classifiers_on_vectors_from_d2v_model(trained_dm_models, top_n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to [Top](#top), [Doc2Vec](#2.-Doc2Vec)\n",
    "#### 2.1.1 Doc2Vec-DM Average F1-scores\n",
    "Scores from previous runs\n",
    "\n",
    "| Window Size                   | 5      | 8      | 10     | 20     |\n",
    "|-------------------------------|--------|--------|--------|--------|\n",
    "| 100 dimension feature vectors | 0.6845 | 0.6886 | 0.6890 | 0.6945 |\n",
    "| 200 dimension feature vectors | 0.6848 | 0.6886 | 0.6918 | 0.6954 |\n",
    "| 300 dimension feature vectors | 0.6859 | 0.6907 | 0.6918 | 0.6954 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Doc2Vec-DM Maximum F1-scores\n",
    "From previous runs\n",
    "\n",
    "| Model                                                            | F1-score | Dimensionality of feature vectors | Window Size |\n",
    "|------------------------------------------------------------------|----------|-----------------------------------|-------------|\n",
    "| ExtraTrees_600, Doc2Vec, model1_dms_d100_hs_w20                  | 0.7067   | 600                               | 20          |\n",
    "| ExtraTrees_600, Doc2Vec, model1_dms_d100_hs_w10                  | 0.7032   | 100                               | 10          |\n",
    "| SGD_l1_penalty, Doc2Vec, model1_dms_d300_hs_w20                  | 0.7027   | 300                               | 20          |\n",
    "\n",
    "Back to [Top](#top), [Doc2Vec](#2.-Doc2Vec), [Doc2Vec-DM](#2.1-Doc2Vec---distributed-memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Doc2Vec - *distributed memory* - 2nd try\n",
    "The [previous attempt](#2.1-Doc2Vec---distributed-memory) just experimented with the window size and the dimensionality of the feature vectors. Next, I manipulate\n",
    "- `sample`, to randomly downsample high frequency words\n",
    "- `negative`, to use negative sampling, i.e. *how many noise words should be drawn*\n",
    "\n",
    "also for different feature vector lengths (100, 200, 300)\n",
    "\n",
    "Best [Average F1-scores](#2.2.1-Doc2Vec---DM-Average-F1-scores,-varying-sample-and-negative) and [Maximum F1-scores](#2.2.2-Doc2Vec---DM-Maximum-F1-scores,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06:44:15: Doc2Vec model 'model2_dms_d200_hs_n8_s0001_w10', Doc2Vec(dm/s,d200,n8,hs,w10,s0.0001,t12)\n",
      "06:44:15: Building vocabulary ...\n",
      "06:44:53: Training ...\n",
      "07:23:54: Saving Doc2Vec model to models/d2v/model2_dms_d200_hs_n8_s0001_w10.doc2vec ...\n",
      "07:24:08: Doc2Vec model 'model2_dms_d100_hs_n10_s0001_w10', Doc2Vec(dm/s,d100,n10,hs,w10,s0.0001,t12)\n",
      "07:24:08: Building vocabulary ...\n",
      "07:24:44: Training ...\n",
      "08:08:05: Saving Doc2Vec model to models/d2v/model2_dms_d100_hs_n10_s0001_w10.doc2vec ...\n",
      "08:08:16: Doc2Vec model 'model2_dms_d100_hs_n8_s001_w10', Doc2Vec(dm/s,d100,n8,hs,w10,s0.001,t12)\n",
      "08:08:16: Building vocabulary ...\n",
      "08:08:50: Training ...\n",
      "08:52:10: Saving Doc2Vec model to models/d2v/model2_dms_d100_hs_n8_s001_w10.doc2vec ...\n",
      "08:52:22: Doc2Vec model 'model2_dms_d200_hs_n8_s001_w10', Doc2Vec(dm/s,d200,n8,hs,w10,s0.001,t12)\n",
      "08:52:22: Building vocabulary ...\n",
      "08:52:57: Training ...\n",
      "09:34:40: Saving Doc2Vec model to models/d2v/model2_dms_d200_hs_n8_s001_w10.doc2vec ...\n",
      "09:35:05: Training classifiers using vectors from Doc2Vec model, model2_dms_d200_hs_n8_s0001_w10\n",
      "09:40:50: ExtraTrees_600, Doc2Vec, model2_dms_d200_hs_n8_s0001_w10, 0.7008\n",
      "09:43:03: LinearSVC_07, Doc2Vec, model2_dms_d200_hs_n8_s0001_w10, 0.7010\n",
      "09:43:13: LogisticRegression, Doc2Vec, model2_dms_d200_hs_n8_s0001_w10, 0.6993\n",
      "09:45:40: LogisticRegressionCV_sag, Doc2Vec, model2_dms_d200_hs_n8_s0001_w10, 0.6995\n",
      "09:45:53: PassiveAggressive_01, Doc2Vec, model2_dms_d200_hs_n8_s0001_w10, 0.6747\n",
      "10:02:25: RandomForest_600, Doc2Vec, model2_dms_d200_hs_n8_s0001_w10, 0.6923\n",
      "10:02:33: RidgeClassifier-auto-1e-3, Doc2Vec, model2_dms_d200_hs_n8_s0001_w10, 0.7012\n",
      "10:03:07: SGD_elasticnet_penalty, Doc2Vec, model2_dms_d200_hs_n8_s0001_w10, 0.7037\n",
      "10:03:37: SGD_l1_penalty, Doc2Vec, model2_dms_d200_hs_n8_s0001_w10, 0.7037\n",
      "10:03:57: SGD_l2_penalty, Doc2Vec, model2_dms_d200_hs_n8_s0001_w10, 0.7035\n",
      "10:03:57: Trained classifiers using vectors from Doc2Vec model, model2_dms_d200_hs_n8_s0001_w10\n",
      "10:03:57: Average F1-score: 0.6980\n",
      "10:04:08: Training classifiers using vectors from Doc2Vec model, model2_dms_d100_hs_n10_s0001_w10\n",
      "10:08:01: ExtraTrees_600, Doc2Vec, model2_dms_d100_hs_n10_s0001_w10, 0.7091\n",
      "10:09:11: LinearSVC_07, Doc2Vec, model2_dms_d100_hs_n10_s0001_w10, 0.6994\n",
      "10:09:19: LogisticRegression, Doc2Vec, model2_dms_d100_hs_n10_s0001_w10, 0.6984\n",
      "10:10:46: LogisticRegressionCV_sag, Doc2Vec, model2_dms_d100_hs_n10_s0001_w10, 0.6984\n",
      "10:10:55: PassiveAggressive_01, Doc2Vec, model2_dms_d100_hs_n10_s0001_w10, 0.6810\n",
      "10:22:27: RandomForest_600, Doc2Vec, model2_dms_d100_hs_n10_s0001_w10, 0.7014\n",
      "10:22:34: RidgeClassifier-auto-1e-3, Doc2Vec, model2_dms_d100_hs_n10_s0001_w10, 0.6995\n",
      "10:22:55: SGD_elasticnet_penalty, Doc2Vec, model2_dms_d100_hs_n10_s0001_w10, 0.7009\n",
      "10:23:14: SGD_l1_penalty, Doc2Vec, model2_dms_d100_hs_n10_s0001_w10, 0.7012\n",
      "10:23:28: SGD_l2_penalty, Doc2Vec, model2_dms_d100_hs_n10_s0001_w10, 0.7011\n",
      "10:23:28: Trained classifiers using vectors from Doc2Vec model, model2_dms_d100_hs_n10_s0001_w10\n",
      "10:23:28: Average F1-score: 0.6990\n",
      "10:23:39: Training classifiers using vectors from Doc2Vec model, model2_dms_d100_hs_n8_s001_w10\n",
      "10:27:27: ExtraTrees_600, Doc2Vec, model2_dms_d100_hs_n8_s001_w10, 0.7049\n",
      "10:28:42: LinearSVC_07, Doc2Vec, model2_dms_d100_hs_n8_s001_w10, 0.6988\n",
      "10:28:51: LogisticRegression, Doc2Vec, model2_dms_d100_hs_n8_s001_w10, 0.6978\n",
      "10:30:19: LogisticRegressionCV_sag, Doc2Vec, model2_dms_d100_hs_n8_s001_w10, 0.6982\n",
      "10:30:29: PassiveAggressive_01, Doc2Vec, model2_dms_d100_hs_n8_s001_w10, 0.6913\n",
      "10:41:53: RandomForest_600, Doc2Vec, model2_dms_d100_hs_n8_s001_w10, 0.6974\n",
      "10:41:59: RidgeClassifier-auto-1e-3, Doc2Vec, model2_dms_d100_hs_n8_s001_w10, 0.6989\n",
      "10:42:21: SGD_elasticnet_penalty, Doc2Vec, model2_dms_d100_hs_n8_s001_w10, 0.6991\n",
      "10:42:41: SGD_l1_penalty, Doc2Vec, model2_dms_d100_hs_n8_s001_w10, 0.6992\n",
      "10:42:55: SGD_l2_penalty, Doc2Vec, model2_dms_d100_hs_n8_s001_w10, 0.6992\n",
      "10:42:55: Trained classifiers using vectors from Doc2Vec model, model2_dms_d100_hs_n8_s001_w10\n",
      "10:42:55: Average F1-score: 0.6985\n",
      "10:43:09: Training classifiers using vectors from Doc2Vec model, model2_dms_d200_hs_n8_s001_w10\n",
      "10:48:53: ExtraTrees_600, Doc2Vec, model2_dms_d200_hs_n8_s001_w10, 0.6967\n",
      "10:50:37: LinearSVC_07, Doc2Vec, model2_dms_d200_hs_n8_s001_w10, 0.7019\n",
      "10:50:47: LogisticRegression, Doc2Vec, model2_dms_d200_hs_n8_s001_w10, 0.7011\n",
      "10:53:15: LogisticRegressionCV_sag, Doc2Vec, model2_dms_d200_hs_n8_s001_w10, 0.7016\n",
      "10:53:27: PassiveAggressive_01, Doc2Vec, model2_dms_d200_hs_n8_s001_w10, 0.6698\n",
      "11:09:49: RandomForest_600, Doc2Vec, model2_dms_d200_hs_n8_s001_w10, 0.6899\n",
      "11:09:56: RidgeClassifier-auto-1e-3, Doc2Vec, model2_dms_d200_hs_n8_s001_w10, 0.7022\n",
      "11:10:30: SGD_elasticnet_penalty, Doc2Vec, model2_dms_d200_hs_n8_s001_w10, 0.7031\n",
      "11:11:00: SGD_l1_penalty, Doc2Vec, model2_dms_d200_hs_n8_s001_w10, 0.7030\n",
      "11:11:19: SGD_l2_penalty, Doc2Vec, model2_dms_d200_hs_n8_s001_w10, 0.7036\n",
      "11:11:19: Trained classifiers using vectors from Doc2Vec model, model2_dms_d200_hs_n8_s001_w10\n",
      "11:11:19: Average F1-score: 0.6973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model                                                               F1-score\n",
      "----------------------------------------------------------------  ----------\n",
      "ExtraTrees_600, Doc2Vec, model2_dms_d100_hs_n10_s0001_w10             0.7091\n",
      "ExtraTrees_600, Doc2Vec, model2_dms_d100_hs_n8_s001_w10               0.7049\n",
      "SGD_elasticnet_penalty, Doc2Vec, model2_dms_d200_hs_n8_s0001_w10      0.7037\n",
      "SGD_l1_penalty, Doc2Vec, model2_dms_d200_hs_n8_s0001_w10              0.7037\n",
      "SGD_l2_penalty, Doc2Vec, model2_dms_d200_hs_n8_s001_w10               0.7036\n"
     ]
    }
   ],
   "source": [
    "# Model 2 : distributed memory (dm=1)\n",
    "\n",
    "# NOTE: All models listed below (including the commented ones) have been run previously.\n",
    "#       Using the uncommented models for demonstration because they have the highest Average F1-scores.\n",
    "\n",
    "models = [\n",
    "    #(\"model2_dms_d100_hs_n5_s0001_w10\", d2v_dim*1, Doc2Vec(alpha=0.025, min_alpha=0.025, min_count=1, negative=5,\n",
    "    #                                                       sample=1e-4, size=d2v_dim*1, window=10, workers=num_cores)),\n",
    "    #(\"model2_dms_d200_hs_n5_s0001_w10\", d2v_dim*2, Doc2Vec(alpha=0.025, min_alpha=0.025, min_count=1, negative=5,\n",
    "    #                                                       sample=1e-4, size=d2v_dim*2, window=10, workers=num_cores)),\n",
    "    #(\"model2_dms_d300_hs_n5_s0001_w10\", d2v_dim*3, Doc2Vec(alpha=0.025, min_alpha=0.025, min_count=1, negative=5,\n",
    "    #                                                       sample=1e-4, size=d2v_dim*3, window=10, workers=num_cores)),\n",
    "    #(\"model2_dms_d100_hs_n8_s0001_w10\", d2v_dim*1, Doc2Vec(alpha=0.025, min_alpha=0.025, min_count=1, negative=8,\n",
    "    #                                                       sample=1e-4, size=d2v_dim*1, window=10, workers=num_cores)),\n",
    "    (\"model2_dms_d200_hs_n8_s0001_w10\", d2v_dim*2, Doc2Vec(alpha=0.025, min_alpha=0.025, min_count=1, negative=8,\n",
    "                                                           sample=1e-4, size=d2v_dim*2, window=10, workers=num_cores)),\n",
    "    #(\"model2_dms_d300_hs_n8_s0001_w10\", d2v_dim*3, Doc2Vec(alpha=0.025, min_alpha=0.025, min_count=1, negative=8,\n",
    "    #                                                       sample=1e-4, size=d2v_dim*3, window=10, workers=num_cores)),\n",
    "    (\"model2_dms_d100_hs_n10_s0001_w10\", d2v_dim*1, Doc2Vec(alpha=0.025, min_alpha=0.025, min_count=1, negative=10,\n",
    "                                                           sample=1e-4, size=d2v_dim*1, window=10, workers=num_cores)),\n",
    "    #(\"model2_dms_d200_hs_n10_s0001_w10\", d2v_dim*2, Doc2Vec(alpha=0.025, min_alpha=0.025, min_count=1, negative=10,\n",
    "    #                                                        sample=1e-4, size=d2v_dim*2, window=10, workers=num_cores)),\n",
    "    #(\"model2_dms_d300_hs_n10_s0001_w10\", d2v_dim*3, Doc2Vec(alpha=0.025, min_alpha=0.025, min_count=1, negative=10,\n",
    "    #                                                        sample=1e-4, size=d2v_dim*3, window=10, workers=num_cores)),\n",
    "    \n",
    "    # Change the sample size\n",
    "    #\n",
    "    #(\"model2_dms_d100_hs_n5_s001_w10\", d2v_dim*1, Doc2Vec(alpha=0.025, min_alpha=0.025, min_count=1, negative=5,\n",
    "    #                                                      sample=1e-3, size=d2v_dim*1, window=10, workers=num_cores)),\n",
    "    #(\"model2_dms_d200_hs_n5_s001_w10\", d2v_dim*2, Doc2Vec(alpha=0.025, min_alpha=0.025, min_count=1, negative=5,\n",
    "    #                                                      sample=1e-3, size=d2v_dim*2, window=10, workers=num_cores)),\n",
    "    #(\"model2_dms_d300_hs_n5_s001_w10\", d2v_dim*3, Doc2Vec(alpha=0.025, min_alpha=0.025, min_count=1, negative=5,\n",
    "    #                                                      sample=1e-3, size=d2v_dim*3, window=10, workers=num_cores)),\n",
    "    #                                                        sample=1e-4, size=d2v_dim*3, window=10, workers=num_cores)),\n",
    "    (\"model2_dms_d100_hs_n8_s001_w10\", d2v_dim*1, Doc2Vec(alpha=0.025, min_alpha=0.025, min_count=1, negative=8,\n",
    "                                                          sample=1e-3, size=d2v_dim*1, window=10, workers=num_cores)),\n",
    "    (\"model2_dms_d200_hs_n8_s001_w10\", d2v_dim*2, Doc2Vec(alpha=0.025, min_alpha=0.025, min_count=1, negative=8,\n",
    "                                                          sample=1e-3, size=d2v_dim*2, window=10, workers=num_cores)),\n",
    "    #(\"model2_dms_d300_hs_n8_s001_w10\", d2v_dim*3, Doc2Vec(alpha=0.025, min_alpha=0.025, min_count=1, negative=8,\n",
    "    #                                                      sample=1e-3, size=d2v_dim*3, window=10, workers=num_cores)),\n",
    "    #(\"model2_dms_d100_hs_n10_s001_w10\", d2v_dim*1, Doc2Vec(alpha=0.025, min_alpha=0.025, min_count=1, negative=10,\n",
    "    #                                                       sample=1e-3, size=d2v_dim*1, window=10, workers=num_cores)),\n",
    "    #(\"model2_dms_d200_hs_n10_s001_w10\", d2v_dim*2, Doc2Vec(alpha=0.025, min_alpha=0.025, min_count=1, negative=10,\n",
    "    #                                                       sample=1e-3, size=d2v_dim*2, window=10, workers=num_cores)),\n",
    "    #(\"model2_dms_d300_hs_n10_s001_w10\", d2v_dim*3, Doc2Vec(alpha=0.025, min_alpha=0.025, min_count=1, negative=10,\n",
    "    #                                                       sample=1e-3, size=d2v_dim*3, window=10, workers=num_cores)),\n",
    "]\n",
    "\n",
    "trained_dm_models_2 = train_doc2vec_models(models) # if skip first x, models[x:]\n",
    "scores_dm_2 = train_classifiers_on_vectors_from_d2v_model(trained_dm_models_2, top_n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to [Top](#top), [Doc2Vec](#2.-Doc2Vec)\n",
    "#### 2.2.1 Doc2Vec - DM Average F1-scores, varying sample and negative\n",
    "Average F1-scores (sample=0.0001, window size fixed at 10) from previous runs\n",
    "\n",
    "| Neagtive Samples              | 5      | 8      | 10     |\n",
    "|-------------------------------|--------|--------|--------|\n",
    "| (sample=0.0001)               |        |        |        |\n",
    "| 100 dimension feature vectors | 0.6955 | 0.6966 | 0.6958 |\n",
    "| 200 dimension feature vectors | 0.6972 | **0.6984** | 0.6965 |\n",
    "| 300 dimension feature vectors | 0.6971 | 0.6981 | 0.6961 |\n",
    "\n",
    "\n",
    "Average F1-scores (sample=0.001, window size fixed at 10) from previous runs\n",
    "\n",
    "| Neagtive Samples              | 5      | 8      | 10     |\n",
    "|-------------------------------|--------|--------|--------|\n",
    "| (sample=0.001)                |        |        |        |\n",
    "| 100 dimension feature vectors | 0.6975 | **0.6992** | 0.6987 |\n",
    "| 200 dimension feature vectors | 0.6984 | **0.6992** | 0.6979 |\n",
    "| 300 dimension feature vectors | 0.6975 | 0.6971 | 0.6969 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Doc2Vec - DM Maximum F1-scores,\n",
    "varying sample and negative (*also comparing with [previous attempt](#2.1-Doc2Vec---distributed-memory) where only window size was varied*)\n",
    "\n",
    "| Model                                                            | F1-score | Dimensionality of feature vectors | Window Size |\n",
    "|------------------------------------------------------------------|----------|-----------------------------------|-------------|\n",
    "| ExtraTrees_600, Doc2Vec, model2_dms_d100_hs_n10_s0001_w10        | 0.7075   | 100                               | 10          |\n",
    "| ExtraTrees_600, Doc2Vec, model2_dms_d100_hs_n8_s0001_w10         | 0.7062   | 100                               | 10          |\n",
    "| SGD_l1_penalty, Doc2Vec, model2_dms_d300_hs_n5_s001_w10          | 0.7060   | 300                               | 10          |\n",
    "| SGD_l1_penalty, Doc2Vec, model2_dms_d300_hs_n8_s0001_w10         | 0.7059   | 300                               | 10          |\n",
    "| SGD_elasticnet_penalty, Doc2Vec, model2_dms_d300_hs_n5_s001_w10  | 0.7057   | 300                               | 10          |\n",
    "| SGD_elasticnet_penalty, Doc2Vec, model2_dms_d300_hs_n10_s001_w10 | 0.7038   | 300                               | 10          |\n",
    "| ExtraTrees_600, Doc2Vec, model1_dms_d100_hs_w10                  | 0.7032   | 100                               | 10          |\n",
    "| SGD_l1_penalty, Doc2Vec, model1_dms_d300_hs_w20                  | 0.7027   | 300                               | 20          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to [Top](#top), [Doc2Vec](#2.-Doc2Vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.3 Doc2Vec - *distributed bag of words*\n",
    "Train the **Doc2Vec-DBOW** models (`dm`=0) and then train the classifiers using the vectors from these models.\n",
    "\n",
    "Training the models take time (30+ minutes), so in most cases I load (from file) previosuly trained models. Set `min_count`=1 because we treat each post as a sentence with a label (example `SENT123`) which appears just once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04:36:17: Doc2Vec model 'PV-DBOW_d100_mc1_n5', Doc2Vec(dbow,d100,n5,t12)\n",
      "04:36:17: Loading from models/d2v/PV-DBOW_d100_mc1_n5.doc2vec ...\n",
      "04:36:21: Doc2Vec model 'PV-DBOW_d300_mc1_n5', Doc2Vec(dbow,d300,n5,t12)\n",
      "04:36:21: Loading from models/d2v/PV-DBOW_d300_mc1_n5.doc2vec ...\n",
      "04:36:30: Doc2Vec model 'PV-DBOW_d300_mc1_n8', Doc2Vec(dbow,d300,n8,t12)\n",
      "04:36:30: Loading from models/d2v/PV-DBOW_d300_mc1_n8.doc2vec ...\n",
      "04:36:39: Doc2Vec model 'PV-DBOW_d100_mc1_n10', Doc2Vec(dbow,d100,n10,t12)\n",
      "04:36:39: Loading from models/d2v/PV-DBOW_d100_mc1_n10.doc2vec ...\n",
      "04:36:51: Training classifiers using vectors from Doc2Vec model, PV-DBOW_d100_mc1_n5\n",
      "04:40:35: ExtraTrees_600, Doc2Vec, PV-DBOW_d100_mc1_n5, 0.7137\n",
      "04:42:08: LinearSVC_07, Doc2Vec, PV-DBOW_d100_mc1_n5, 0.7035\n",
      "04:42:15: LogisticRegression, Doc2Vec, PV-DBOW_d100_mc1_n5, 0.7033\n",
      "04:43:43: LogisticRegressionCV_sag, Doc2Vec, PV-DBOW_d100_mc1_n5, 0.7033\n",
      "04:43:52: PassiveAggressive_01, Doc2Vec, PV-DBOW_d100_mc1_n5, 0.6665\n",
      "04:55:02: RandomForest_600, Doc2Vec, PV-DBOW_d100_mc1_n5, 0.7084\n",
      "04:55:08: RidgeClassifier-auto-1e-3, Doc2Vec, PV-DBOW_d100_mc1_n5, 0.7033\n",
      "04:55:29: SGD_elasticnet_penalty, Doc2Vec, PV-DBOW_d100_mc1_n5, 0.6964\n",
      "04:55:47: SGD_l1_penalty, Doc2Vec, PV-DBOW_d100_mc1_n5, 0.6968\n",
      "04:56:01: SGD_l2_penalty, Doc2Vec, PV-DBOW_d100_mc1_n5, 0.6959\n",
      "04:56:01: Trained classifiers using vectors from Doc2Vec model, PV-DBOW_d100_mc1_n5\n",
      "04:56:01: Average F1-score: 0.6991\n",
      "04:56:12: Training classifiers using vectors from Doc2Vec model, PV-DBOW_d300_mc1_n5\n",
      "05:03:09: ExtraTrees_600, Doc2Vec, PV-DBOW_d300_mc1_n5, 0.6971\n",
      "05:05:56: LinearSVC_07, Doc2Vec, PV-DBOW_d300_mc1_n5, 0.7109\n",
      "05:06:08: LogisticRegression, Doc2Vec, PV-DBOW_d300_mc1_n5, 0.7102\n",
      "05:09:19: LogisticRegressionCV_sag, Doc2Vec, PV-DBOW_d300_mc1_n5, 0.7107\n",
      "05:09:33: PassiveAggressive_01, Doc2Vec, PV-DBOW_d300_mc1_n5, 0.6692\n",
      "05:28:38: RandomForest_600, Doc2Vec, PV-DBOW_d300_mc1_n5, 0.6927\n",
      "05:28:46: RidgeClassifier-auto-1e-3, Doc2Vec, PV-DBOW_d300_mc1_n5, 0.7112\n",
      "05:29:32: SGD_elasticnet_penalty, Doc2Vec, PV-DBOW_d300_mc1_n5, 0.7077\n",
      "05:30:14: SGD_l1_penalty, Doc2Vec, PV-DBOW_d300_mc1_n5, 0.7076\n",
      "05:30:38: SGD_l2_penalty, Doc2Vec, PV-DBOW_d300_mc1_n5, 0.7073\n",
      "05:30:38: Trained classifiers using vectors from Doc2Vec model, PV-DBOW_d300_mc1_n5\n",
      "05:30:38: Average F1-score: 0.7024\n",
      "05:30:48: Training classifiers using vectors from Doc2Vec model, PV-DBOW_d300_mc1_n8\n",
      "05:37:48: ExtraTrees_600, Doc2Vec, PV-DBOW_d300_mc1_n8, 0.6951\n",
      "05:40:44: LinearSVC_07, Doc2Vec, PV-DBOW_d300_mc1_n8, 0.7090\n",
      "05:40:56: LogisticRegression, Doc2Vec, PV-DBOW_d300_mc1_n8, 0.7082\n",
      "05:44:25: LogisticRegressionCV_sag, Doc2Vec, PV-DBOW_d300_mc1_n8, 0.7094\n",
      "05:44:40: PassiveAggressive_01, Doc2Vec, PV-DBOW_d300_mc1_n8, 0.6601\n",
      "06:03:39: RandomForest_600, Doc2Vec, PV-DBOW_d300_mc1_n8, 0.6920\n",
      "06:03:47: RidgeClassifier-auto-1e-3, Doc2Vec, PV-DBOW_d300_mc1_n8, 0.7095\n",
      "06:04:34: SGD_elasticnet_penalty, Doc2Vec, PV-DBOW_d300_mc1_n8, 0.7036\n",
      "06:05:18: SGD_l1_penalty, Doc2Vec, PV-DBOW_d300_mc1_n8, 0.7042\n",
      "06:05:42: SGD_l2_penalty, Doc2Vec, PV-DBOW_d300_mc1_n8, 0.7022\n",
      "06:05:42: Trained classifiers using vectors from Doc2Vec model, PV-DBOW_d300_mc1_n8\n",
      "06:05:42: Average F1-score: 0.6993\n",
      "06:05:49: Training classifiers using vectors from Doc2Vec model, PV-DBOW_d100_mc1_n10\n",
      "06:09:33: ExtraTrees_600, Doc2Vec, PV-DBOW_d100_mc1_n10, 0.7151\n",
      "06:11:02: LinearSVC_07, Doc2Vec, PV-DBOW_d100_mc1_n10, 0.7034\n",
      "06:11:09: LogisticRegression, Doc2Vec, PV-DBOW_d100_mc1_n10, 0.7035\n",
      "06:12:34: LogisticRegressionCV_sag, Doc2Vec, PV-DBOW_d100_mc1_n10, 0.7044\n",
      "06:12:42: PassiveAggressive_01, Doc2Vec, PV-DBOW_d100_mc1_n10, 0.6553\n",
      "06:23:48: RandomForest_600, Doc2Vec, PV-DBOW_d100_mc1_n10, 0.7087\n",
      "06:23:54: RidgeClassifier-auto-1e-3, Doc2Vec, PV-DBOW_d100_mc1_n10, 0.7034\n",
      "06:24:14: SGD_elasticnet_penalty, Doc2Vec, PV-DBOW_d100_mc1_n10, 0.6942\n",
      "06:24:33: SGD_l1_penalty, Doc2Vec, PV-DBOW_d100_mc1_n10, 0.6935\n",
      "06:24:46: SGD_l2_penalty, Doc2Vec, PV-DBOW_d100_mc1_n10, 0.6935\n",
      "06:24:46: Trained classifiers using vectors from Doc2Vec model, PV-DBOW_d100_mc1_n10\n",
      "06:24:46: Average F1-score: 0.6975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model                                                      F1-score\n",
      "-------------------------------------------------------  ----------\n",
      "ExtraTrees_600, Doc2Vec, PV-DBOW_d100_mc1_n10                0.7151\n",
      "ExtraTrees_600, Doc2Vec, PV-DBOW_d100_mc1_n5                 0.7137\n",
      "RidgeClassifier-auto-1e-3, Doc2Vec, PV-DBOW_d300_mc1_n5      0.7112\n",
      "LinearSVC_07, Doc2Vec, PV-DBOW_d300_mc1_n5                   0.7109\n",
      "LogisticRegressionCV_sag, Doc2Vec, PV-DBOW_d300_mc1_n5       0.7107\n",
      "LogisticRegression, Doc2Vec, PV-DBOW_d300_mc1_n5             0.7102\n",
      "RidgeClassifier-auto-1e-3, Doc2Vec, PV-DBOW_d300_mc1_n8      0.7095\n",
      "LogisticRegressionCV_sag, Doc2Vec, PV-DBOW_d300_mc1_n8       0.7094\n",
      "LinearSVC_07, Doc2Vec, PV-DBOW_d300_mc1_n8                   0.7090\n",
      "RandomForest_600, Doc2Vec, PV-DBOW_d100_mc1_n10              0.7087\n"
     ]
    }
   ],
   "source": [
    "# Train the `distributed bag of words` (PV-DBOW) models\n",
    "\n",
    "# NOTE: All models listed below (including the commented ones) have been run previously.\n",
    "#       Using the uncommented models for demonstration because they have the highest Average F1-scores\n",
    "#       or a high individual F1-score.\n",
    "\n",
    "models = [\n",
    "    (\"PV-DBOW_d100_mc1_n5\", d2v_dim*1, Doc2Vec(dm=0, hs=0, min_count=1, negative=5, size=d2v_dim*1, workers=num_cores)),\n",
    "    #(\"PV-DBOW_d200_mc1_n5\", d2v_dim*2, Doc2Vec(dm=0, hs=0, min_count=1, negative=5, size=d2v_dim*2, workers=num_cores)),\n",
    "    (\"PV-DBOW_d300_mc1_n5\", d2v_dim*3, Doc2Vec(dm=0, hs=0, min_count=1, negative=5, size=d2v_dim*3, workers=num_cores)),\n",
    "    #(\"PV-DBOW_d100_mc1_n8\", d2v_dim*1, Doc2Vec(dm=0, hs=0, min_count=1, negative=8, size=d2v_dim*1, workers=num_cores)),\n",
    "    #(\"PV-DBOW_d200_mc1_n8\", d2v_dim*2, Doc2Vec(dm=0, hs=0, min_count=1, negative=8, size=d2v_dim*2, workers=num_cores)),\n",
    "    (\"PV-DBOW_d300_mc1_n8\", d2v_dim*3, Doc2Vec(dm=0, hs=0, min_count=1, negative=8, size=d2v_dim*3, workers=num_cores)),\n",
    "    (\"PV-DBOW_d100_mc1_n10\", d2v_dim*1, Doc2Vec(dm=0, hs=0, min_count=1, negative=10, size=d2v_dim*1, workers=num_cores)),\n",
    "    #(\"PV-DBOW_d200_mc1_n10\", d2v_dim*2, Doc2Vec(dm=0, hs=0, min_count=1, negative=10, size=d2v_dim*2, workers=num_cores)),\n",
    "    #(\"PV-DBOW_d300_mc1_n10\", d2v_dim*3, Doc2Vec(dm=0, hs=0, min_count=1, negative=10, size=d2v_dim*3, workers=num_cores))\n",
    "]\n",
    "trained_dbow_models = train_doc2vec_models(models)\n",
    "\n",
    "# Train the classifiers using the vectors from the Doc2Vec-DBOW models\n",
    "scores_dbow_1 = train_classifiers_on_vectors_from_d2v_model(trained_dbow_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average and Maximum F1-scores from previous runs\n",
    "\n",
    "- n=5,\n",
    "  - 100 dimensions, Average F1-score: 0.6984, maximum : ExtraTrees_600, Doc2Vec, PV-DBOW_d100_mc1_n5, **0.7152**\n",
    "  - 200 dimensions, Average F1-score: 0.6998, maximum : RidgeClassifier-auto-1e-3, Doc2Vec, PV-DBOW_d200_mc1_n5, 0.7062\n",
    "  - 300 dimensions, Average F1-score: **0.7013**, maximum : RidgeClassifier-auto-1e-3, Doc2Vec, PV-DBOW_d300_mc1_n5, 0.7111\n",
    "- n=8,\n",
    "  - 100 dimensions, Average F1-score: 0.6969, maximum : ExtraTrees_600, Doc2Vec, PV-DBOW_d100_mc1_n8, 0.7150\n",
    "  - 200 dimensions, Average F1-score: 0.6985, maximum : LinearSVC_07, Doc2Vec, PV-DBOW_d200_mc1_n8, 0.7057\n",
    "  - 300 dimensions, Average F1-score: **0.7011**, maximum : LinearSVC_07, Doc2Vec, PV-DBOW_d300_mc1_n8, 0.7088\n",
    "- n=10,\n",
    "  - 100 dimensions, Average F1-score: 0.6970, maximum : ExtraTrees_600, Doc2Vec, PV-DBOW_d100_mc1_n10, **0.7160**\n",
    "  - 200 dimensions, Average F1-score: 0.6976, maximum : RidgeClassifier-auto-1e-3, Doc2Vec, PV-DBOW_d200_mc1_n10, 0.7054\n",
    "  - 300 dimensoons, Average F1-score: 0.6996, maximum : RidgeClassifier-auto-1e-3, Doc2Vec, PV-DBOW_d300_mc1_n10, 0.7088"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to [Top](#top), [Doc2Vec](#2.-Doc2Vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3. Final Comparison\n",
    "For this dataset, \n",
    "- *Distributed bag of words* (PV-DBOW) marginally better than *distributed memory* (PV-DM) models (0.7151 vs 0.7091)\n",
    "- Most *Bag of Words* (BoW) approaches score higher than Doc2Vec and Word2Vec approaches - was expecting otherwise.\n",
    "  - 0.7307, BoW \n",
    "  - 0.7151, Doc2Vec\n",
    "  - 0.6875, Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model                                                        F1-score\n",
      "---------------------------------------------------------  ----------\n",
      "ExtraTrees_600, Doc2Vec, PV-DBOW_d100_mc1_n10                  0.7151\n",
      "ExtraTrees_600, Doc2Vec, PV-DBOW_d100_mc1_n5                   0.7137\n",
      "RidgeClassifier-auto-1e-3, Doc2Vec, PV-DBOW_d300_mc1_n5        0.7112\n",
      "LinearSVC_07, Doc2Vec, PV-DBOW_d300_mc1_n5                     0.7109\n",
      "LogisticRegressionCV_sag, Doc2Vec, PV-DBOW_d300_mc1_n5         0.7107\n",
      "LogisticRegression, Doc2Vec, PV-DBOW_d300_mc1_n5               0.7102\n",
      "RidgeClassifier-auto-1e-3, Doc2Vec, PV-DBOW_d300_mc1_n8        0.7095\n",
      "LogisticRegressionCV_sag, Doc2Vec, PV-DBOW_d300_mc1_n8         0.7094\n",
      "ExtraTrees_600, Doc2Vec, model2_dms_d100_hs_n10_s0001_w10      0.7091\n",
      "LinearSVC_07, Doc2Vec, PV-DBOW_d300_mc1_n8                     0.7090\n"
     ]
    }
   ],
   "source": [
    "top_scores = scores_dm_1 + scores_dm_2 + scores_dbow_1\n",
    "top_scores = sorted(top_scores, key=lambda x: -x[1])\n",
    "print(tabulate(top_scores[:20], floatfmt=\".4f\", headers=(\"Model\", \"F1-score\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Back to [Top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
